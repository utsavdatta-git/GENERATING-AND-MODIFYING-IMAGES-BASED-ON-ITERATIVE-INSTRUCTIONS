{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating images from scene graphs.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavdatta-git/GENERATING-AND-MODIFYING-IMAGES-BASED-ON-ITERATIVE-INSTRUCTIONS/blob/master/Generating_images_from_scene_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L0U_nGr94Ia",
        "colab_type": "text"
      },
      "source": [
        "# Image Generation from Scene Graphs\n",
        "***By Justin Johnson, Agrim Gupta, Li Fei-Fei***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRL5oVC-fJp",
        "colab_type": "text"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo_Thxqd-rig",
        "colab_type": "text"
      },
      "source": [
        "### Clone code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOsD3gpRjqqb",
        "colab_type": "code",
        "outputId": "14347dd7-1d79-40b2-84ef-604d3125d4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#@title Cloning sg2im github repo {display-mode: \"form\"}\n",
        "!git clone https://github.com/google/sg2im.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sg2im'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Total 85 (delta 0), reused 0 (delta 0), pack-reused 85\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_eLBKXk-1T2",
        "colab_type": "text"
      },
      "source": [
        "### Download pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ululMJAY0jm7",
        "colab_type": "code",
        "outputId": "47d20dd2-e55e-4510-94d8-bbd52ff80a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title downloading pretrained models {display-mode: \"form\"}\n",
        "!sh \"/content/sg2im/scripts/download_full_models.sh\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-12 23:52:38--  https://storage.googleapis.com/sg2im-data/full/coco64.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c13::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 809109670 (772M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 771.63M  97.0MB/s    in 8.4s    \n",
            "\n",
            "2019-11-12 23:52:47 (92.2 MB/s) - ‘sg2im-models/full/coco64.pt’ saved [809109670/809109670]\n",
            "\n",
            "--2019-11-12 23:52:47--  https://storage.googleapis.com/sg2im-data/full/coco64_no_gconv.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472158519 (450M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_no_gconv.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 450.29M   116MB/s    in 4.1s    \n",
            "\n",
            "2019-11-12 23:52:51 (109 MB/s) - ‘sg2im-models/full/coco64_no_gconv.pt’ saved [472158519/472158519]\n",
            "\n",
            "--2019-11-12 23:52:52--  https://storage.googleapis.com/sg2im-data/full/coco64_no_relations.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 539243251 (514M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_no_relations.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 514.26M  90.1MB/s    in 6.0s    \n",
            "\n",
            "2019-11-12 23:52:58 (86.4 MB/s) - ‘sg2im-models/full/coco64_no_relations.pt’ saved [539243251/539243251]\n",
            "\n",
            "--2019-11-12 23:52:58--  https://storage.googleapis.com/sg2im-data/full/coco64_no_discriminators.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786173546 (750M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_no_discriminators.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 749.75M   125MB/s    in 7.7s    \n",
            "\n",
            "2019-11-12 23:53:06 (97.6 MB/s) - ‘sg2im-models/full/coco64_no_discriminators.pt’ saved [786173546/786173546]\n",
            "\n",
            "--2019-11-12 23:53:06--  https://storage.googleapis.com/sg2im-data/full/coco64_no_img_discriminator.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.123.128, 2607:f8b0:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.123.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800454204 (763M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_no_img_discriminator.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 763.37M  91.0MB/s    in 8.7s    \n",
            "\n",
            "2019-11-12 23:53:15 (87.6 MB/s) - ‘sg2im-models/full/coco64_no_img_discriminator.pt’ saved [800454204/800454204]\n",
            "\n",
            "--2019-11-12 23:53:15--  https://storage.googleapis.com/sg2im-data/full/coco64_no_obj_discriminator.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c15::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 793384900 (757M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_no_obj_discriminator.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 756.63M  75.7MB/s    in 8.9s    \n",
            "\n",
            "2019-11-12 23:53:24 (84.6 MB/s) - ‘sg2im-models/full/coco64_no_obj_discriminator.pt’ saved [793384900/793384900]\n",
            "\n",
            "--2019-11-12 23:53:24--  https://storage.googleapis.com/sg2im-data/full/coco64_gt_layout.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 2607:f8b0:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444003064 (423M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_gt_layout.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 423.43M  95.6MB/s    in 4.5s    \n",
            "\n",
            "2019-11-12 23:53:29 (93.5 MB/s) - ‘sg2im-models/full/coco64_gt_layout.pt’ saved [444003064/444003064]\n",
            "\n",
            "--2019-11-12 23:53:29--  https://storage.googleapis.com/sg2im-data/full/coco64_gt_layout_no_gconv.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.123.128, 2607:f8b0:400c:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.123.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 376896268 (359M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/coco64_gt_layout_no_gconv.pt’\n",
            "\n",
            "sg2im-models/full/c 100%[===================>] 359.44M  64.6MB/s    in 5.7s    \n",
            "\n",
            "2019-11-12 23:53:34 (63.6 MB/s) - ‘sg2im-models/full/coco64_gt_layout_no_gconv.pt’ saved [376896268/376896268]\n",
            "\n",
            "--2019-11-12 23:53:35--  https://storage.googleapis.com/sg2im-data/full/vg64.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c15::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511142739 (487M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg64.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 487.46M  90.8MB/s    in 5.6s    \n",
            "\n",
            "2019-11-12 23:53:40 (87.2 MB/s) - ‘sg2im-models/full/vg64.pt’ saved [511142739/511142739]\n",
            "\n",
            "--2019-11-12 23:53:40--  https://storage.googleapis.com/sg2im-data/full/vg128.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 2607:f8b0:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 942767583 (899M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg128.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 899.09M  90.3MB/s    in 11s     \n",
            "\n",
            "2019-11-12 23:53:51 (84.2 MB/s) - ‘sg2im-models/full/vg128.pt’ saved [942767583/942767583]\n",
            "\n",
            "--2019-11-12 23:53:51--  https://storage.googleapis.com/sg2im-data/full/vg64_no_relations.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 2607:f8b0:400c:c10::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511052099 (487M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg64_no_relations.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 487.38M   101MB/s    in 5.1s    \n",
            "\n",
            "2019-11-12 23:53:56 (95.7 MB/s) - ‘sg2im-models/full/vg64_no_relations.pt’ saved [511052099/511052099]\n",
            "\n",
            "--2019-11-12 23:53:56--  https://storage.googleapis.com/sg2im-data/full/vg64_no_gconv.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 2607:f8b0:400c:c15::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444204833 (424M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg64_no_gconv.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 423.63M   110MB/s    in 3.9s    \n",
            "\n",
            "2019-11-12 23:54:01 (110 MB/s) - ‘sg2im-models/full/vg64_no_gconv.pt’ saved [444204833/444204833]\n",
            "\n",
            "--2019-11-12 23:54:01--  https://storage.googleapis.com/sg2im-data/full/vg64_no_discriminators.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 2607:f8b0:400c:c10::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 488805112 (466M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg64_no_discriminators.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 466.16M  39.3MB/s    in 22s     \n",
            "\n",
            "2019-11-12 23:54:23 (20.9 MB/s) - ‘sg2im-models/full/vg64_no_discriminators.pt’ saved [488805112/488805112]\n",
            "\n",
            "--2019-11-12 23:54:23--  https://storage.googleapis.com/sg2im-data/full/vg64_no_img_discriminator.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 2607:f8b0:400c:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 497102933 (474M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg64_no_img_discriminator.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 474.07M  81.3MB/s    in 5.9s    \n",
            "\n",
            "2019-11-12 23:54:30 (79.7 MB/s) - ‘sg2im-models/full/vg64_no_img_discriminator.pt’ saved [497102933/497102933]\n",
            "\n",
            "--2019-11-12 23:54:30--  https://storage.googleapis.com/sg2im-data/full/vg64_no_obj_discriminator.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 2607:f8b0:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 441954013 (421M) [application/octet-stream]\n",
            "Saving to: ‘sg2im-models/full/vg64_no_obj_discriminator.pt’\n",
            "\n",
            "sg2im-models/full/v 100%[===================>] 421.48M  59.9MB/s    in 8.9s    \n",
            "\n",
            "2019-11-12 23:54:39 (47.3 MB/s) - ‘sg2im-models/full/vg64_no_obj_discriminator.pt’ saved [441954013/441954013]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ibdDcT1uBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "os.chdir(\"/content/sg2im/\")\n",
        "sys.path.append(\"/content/sg2im/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj_pTRQezS8-",
        "colab_type": "text"
      },
      "source": [
        "### Download Scene Graph parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIGno0CC91QD",
        "colab_type": "code",
        "outputId": "bef26aa7-5b93-4241-a0a5-63e40137bcf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip\n",
        "!wget https://nlp.stanford.edu/projects/scenegraph/scenegraph-1.0.jar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-12 23:55:01--  https://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 403157240 (384M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2015-12-09.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 384.48M  7.88MB/s    in 23s     \n",
            "\n",
            "2019-11-12 23:55:24 (16.5 MB/s) - ‘stanford-corenlp-full-2015-12-09.zip’ saved [403157240/403157240]\n",
            "\n",
            "--2019-11-12 23:55:26--  https://nlp.stanford.edu/projects/scenegraph/scenegraph-1.0.jar\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 164039 (160K) [application/x-java-archive]\n",
            "Saving to: ‘scenegraph-1.0.jar’\n",
            "\n",
            "scenegraph-1.0.jar  100%[===================>] 160.19K   379KB/s    in 0.4s    \n",
            "\n",
            "2019-11-12 23:55:26 (379 KB/s) - ‘scenegraph-1.0.jar’ saved [164039/164039]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdsYu22M-bQZ",
        "colab_type": "code",
        "outputId": "2fad6083-6831-453c-96f1-a25e9588555b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        }
      },
      "source": [
        "!unzip /content/sg2im/stanford-corenlp-full-2015-12-09.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/sg2im/stanford-corenlp-full-2015-12-09.zip\n",
            "   creating: stanford-corenlp-full-2015-12-09/\n",
            "  inflating: stanford-corenlp-full-2015-12-09/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/README.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2015-12-09/sutime/\n",
            "  inflating: stanford-corenlp-full-2015-12-09/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2015-12-09/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/build.xml  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0.jar  \n",
            "   creating: stanford-corenlp-full-2015-12-09/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2015-12-09/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2015-12-09/patterns/\n",
            "  inflating: stanford-corenlp-full-2015-12-09/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2015-12-09/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2015-12-09/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2015-12-09/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/input.txt  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/input.txt.out  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/jollyday-0.4.7-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/Makefile  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0-models.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/stanford-corenlp-3.6.0-javadoc.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2015-12-09/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLK2toO8-5aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/sg2im/scenegraph-1.0.jar /content/sg2im/stanford-corenlp-full-2015-12-09"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCGqWns-eoor",
        "colab_type": "code",
        "outputId": "80bd6b84-7cf4-468c-f349-cee61f3fe023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Downloading code from google drive\n",
        "file_ids = [('1pwiIV3VhxLtLbChwdCiRK284xOGNW9e8', 'TextToSG.class'),\n",
        "           ('1Lp2UrBxA5L12fXYgS49arC8SIzLpIyBM', 'json-simple-1.1.jar')] # URL id.\n",
        "for file_id, file_name in file_ids:\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(file_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.7)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.4.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QSuuC3Bg0pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/sg2im/json-simple-1.1.jar /content/sg2im/stanford-corenlp-full-2015-12-09\n",
        "!mv /content/sg2im/TextToSG.class /content/sg2im/stanford-corenlp-full-2015-12-09"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRvG8U0i--Kv",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DZcfgGUbAhx",
        "colab_type": "text"
      },
      "source": [
        "### Parse text to scene graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pscnrag-_iVg",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "9011255d-25bb-4142-f091-3e67b3f546f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "#@title Model Parameters\n",
        "checkpoint = \"/content/sg2im-models/full/vg128.pt\" #@param [\"/content/sg2im-models/full/coco64.pt\", \"/content/sg2im-models/full/vg128.pt\", \"/content/sg2im-models/full/vg64.pt\"]\n",
        "scene_graphs_json = 'scene_graphs/scenegraph.json' #@param {type:\"string\"}\n",
        "output_dir = '/content/output' #@param {type:\"string\"}\n",
        "draw_scene_graphs = 1 #@param {type:\"integer\"}\n",
        "device = \"gpu\" #@param [\"gpu\", \"cpu\"]\n",
        "text_to_parse = \"A man by a sheep.\" #@param {type:\"string\"}\n",
        "text_to_parse_java = '\"'+text_to_parse+'\"'\n",
        "\n",
        "os.chdir(\"/content/sg2im/stanford-corenlp-full-2015-12-09\")\n",
        "!/usr/lib/jvm/java-8-openjdk-amd64/bin/java -mx2g -cp .:\"*\" TextToSG $text_to_parse_java\n",
        "\n",
        "with open(\"/content/sg2im/stanford-corenlp-full-2015-12-09/scenegraph.json\", 'r') as sg:\n",
        "  graph = json.load(sg)\n",
        "  relationships = [[relation['subject'],relation['predicate'], relation['object']] for relation in graph['relationships']]\n",
        "  objects = [object[\"names\"][0] for object in graph['objects']]\n",
        "  scene_graph = {}\n",
        "  scene_graph['relationships'] = relationships\n",
        "  scene_graph['objects'] = objects\n",
        "  with open('/content/sg2im/scene_graphs/scenegraph.json', 'w') as sg_out:\n",
        "    sg_out.write(json.dumps([scene_graph]))\n",
        "\n",
        "os.chdir(\"/content/sg2im/\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
            "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
            "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
            "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
            "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
            "done [0.6 sec].\n",
            "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
            "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
            "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.6 sec].\n",
            "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
            "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.1 sec].\n",
            "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
            "Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt\n",
            "Nov 13, 2019 12:19:50 AM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n",
            "INFO: Read 83 rules\n",
            "Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt\n",
            "Nov 13, 2019 12:19:50 AM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n",
            "INFO: Read 267 rules\n",
            "Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\n",
            "Nov 13, 2019 12:19:50 AM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules\n",
            "INFO: Read 25 rules\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OADoRN4qcgA-",
        "colab_type": "text"
      },
      "source": [
        "### Generated image from parsed scene graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCPJ1Tzq3LlN",
        "colab_type": "code",
        "outputId": "2118cd8b-5007-46b1-a083-fdf27fe7d39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "#@title Visualize results {display-mode: \"form\"}\n",
        "#!/usr/bin/python\n",
        "#\n",
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import argparse, json, os\n",
        "\n",
        "from imageio import imwrite\n",
        "import torch\n",
        "\n",
        "from sg2im.model import Sg2ImModel\n",
        "from sg2im.data.utils import imagenet_deprocess_batch\n",
        "import sg2im.vis as vis\n",
        "\n",
        "class argsClass:\n",
        "  def __init__(self, checkpoint='sg2im-models/vg128.pt',\n",
        "               scene_graphs_json='scene_graphs/figure_6_sheep.json',\n",
        "               output_dir='outputs',\n",
        "               draw_scene_graphs=0,\n",
        "               device='gpu'):\n",
        "    self.checkpoint = checkpoint\n",
        "    self.scene_graphs_json=scene_graphs_json\n",
        "    self.output_dir=output_dir\n",
        "    self.draw_scene_graphs=draw_scene_graphs\n",
        "    self.device=device\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  if not os.path.isfile(args.checkpoint):\n",
        "    print('ERROR: Checkpoint file \"%s\" not found' % args.checkpoint)\n",
        "    print('Maybe you forgot to download pretraind models? Try running:')\n",
        "    print('bash scripts/download_models.sh')\n",
        "    return\n",
        "\n",
        "  if not os.path.isdir(args.output_dir):\n",
        "    print('Output directory \"%s\" does not exist; creating it' % args.output_dir)\n",
        "    os.makedirs(args.output_dir)\n",
        "\n",
        "  if args.device == 'cpu':\n",
        "    device = torch.device('cpu')\n",
        "  elif args.device == 'gpu':\n",
        "    device = torch.device('cuda:0')\n",
        "    if not torch.cuda.is_available():\n",
        "      print('WARNING: CUDA not available; falling back to CPU')\n",
        "      device = torch.device('cpu')\n",
        "\n",
        "  # Load the model, with a bit of care in case there are no GPUs\n",
        "  map_location = 'cpu' if device == torch.device('cpu') else None\n",
        "  checkpoint = torch.load(args.checkpoint, map_location=map_location)\n",
        "  model = Sg2ImModel(**checkpoint['model_kwargs'])\n",
        "  model.load_state_dict(checkpoint['model_state'])\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "\n",
        "  # Load the scene graphs\n",
        "  with open(args.scene_graphs_json, 'r') as f:\n",
        "    scene_graphs = json.load(f)\n",
        "\n",
        "  # Run the model forward\n",
        "  with torch.no_grad():\n",
        "    imgs, boxes_pred, masks_pred, _ = model.forward_json(scene_graphs)\n",
        "  imgs = imagenet_deprocess_batch(imgs)\n",
        "\n",
        "  # Save the generated images\n",
        "  for i in range(imgs.shape[0]):\n",
        "    img_np = imgs[i].numpy().transpose(1, 2, 0)\n",
        "    img_path = os.path.join(args.output_dir, 'img%06d.png' % i)\n",
        "    imwrite(img_path, img_np)\n",
        "\n",
        "  # Draw the scene graphs\n",
        "  if args.draw_scene_graphs == 1:\n",
        "    for i, sg in enumerate(scene_graphs):\n",
        "      sg_img = vis.draw_scene_graph(sg['objects'], sg['relationships'])\n",
        "      sg_img_path = os.path.join(args.output_dir, 'sg%06d.png' % i)\n",
        "      imwrite(sg_img_path, sg_img)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  args = argsClass(checkpoint=checkpoint,\n",
        "                   scene_graphs_json=scene_graphs_json,\n",
        "                   output_dir=output_dir,\n",
        "                   draw_scene_graphs=draw_scene_graphs,\n",
        "                   device=device)\n",
        "  main(args)\n",
        "  from IPython.display import clear_output\n",
        "  clear_output()\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  print(text_to_parse)\n",
        "  with open(scene_graphs_json, 'r') as sg_json:\n",
        "    sgs = json.load(sg_json)\n",
        "    #plt.subplots(2, len(sgs))\n",
        "    plt.figure(figsize=(10 * len(sgs),5 * len(sgs)))\n",
        "    plt.axis('off')\n",
        "    for i in range(len(sgs)):\n",
        "      plt.subplot(len(sgs), 2 ,2 * i + 1)\n",
        "      img = plt.imread(output_dir + '/img00000'+str(i)+'.png')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "      plt.subplot(len(sgs), 2,2 * i + 2)\n",
        "      img = plt.imread(output_dir + '/sg00000'+str(i)+'.png')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A man by a sheep.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAEeCAYAAAD4uX2xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK7wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JF1pJLb_H8W",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjpMYVFoXk6r",
        "colab_type": "text"
      },
      "source": [
        "#### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGo7cMjdYnWQ",
        "colab_type": "code",
        "outputId": "84c1adb0-e8d3-4e5a-cca8-0bf68967dcb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "os.chdir(\"/content/cocoapi/PythonAPI/\")\n",
        "!python setup.py install\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 959, done.\u001b[K\n",
            "remote: Total 959 (delta 0), reused 0 (delta 0), pack-reused 959\u001b[K\n",
            "Receiving objects: 100% (959/959), 11.69 MiB | 33.71 MiB/s, done.\n",
            "Resolving deltas: 100% (570/570), done.\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating pycocotools.egg-info\n",
            "writing pycocotools.egg-info/PKG-INFO\n",
            "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
            "writing requirements to pycocotools.egg-info/requires.txt\n",
            "writing top-level names to pycocotools.egg-info/top_level.txt\n",
            "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
            "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/pycocotools\n",
            "copying pycocotools/coco.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
            "copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
            "copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
            "copying pycocotools/mask.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.6/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.6/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/../common/maskApi.o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -o build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.6/pycocotools/coco.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.6/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.6/pycocotools/__init__.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "copying build/lib.linux-x86_64-3.6/pycocotools/mask.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/coco.py to coco.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/mask.py to mask.cpython-36.pyc\n",
            "creating stub loader for pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/_mask.py to _mask.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pycocotools.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "pycocotools.__pycache__._mask.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/pycocotools-2.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pycocotools-2.0-py3.6-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/pycocotools-2.0-py3.6-linux-x86_64.egg\n",
            "Extracting pycocotools-2.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pycocotools 2.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pycocotools-2.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for pycocotools==2.0\n",
            "Searching for matplotlib==3.0.3\n",
            "Best match: matplotlib 3.0.3\n",
            "Adding matplotlib 3.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Cython==0.29.13\n",
            "Best match: Cython 0.29.13\n",
            "Adding Cython 0.29.13 to easy-install.pth file\n",
            "Installing cygdb script to /usr/local/bin\n",
            "Installing cython script to /usr/local/bin\n",
            "Installing cythonize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==41.2.0\n",
            "Best match: setuptools 41.2.0\n",
            "Adding setuptools 41.2.0 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.2\n",
            "Best match: pyparsing 2.4.2\n",
            "Adding pyparsing 2.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.16.5\n",
            "Best match: numpy 1.16.5\n",
            "Adding numpy 1.16.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.5.3\n",
            "Best match: python-dateutil 2.5.3\n",
            "Adding python-dateutil 2.5.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.1.0\n",
            "Best match: kiwisolver 1.1.0\n",
            "Adding kiwisolver 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for pycocotools==2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE_F29FxrNRZ",
        "colab_type": "code",
        "outputId": "7e995e7a-ff67-492d-899c-1ac812a6a405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.chdir(\"/content/sg2im/\")\n",
        "!sh scripts/download_vg.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 05:59:40--  https://visualgenome.org/static/data/dataset/objects.json.zip\n",
            "Resolving visualgenome.org (visualgenome.org)... 54.87.32.87\n",
            "Connecting to visualgenome.org (visualgenome.org)|54.87.32.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55323929 (53M) [application/zip]\n",
            "Saving to: ‘datasets/vg/objects.json.zip’\n",
            "\n",
            "datasets/vg/objects 100%[===================>]  52.76M  53.7MB/s    in 1.0s    \n",
            "\n",
            "2019-10-15 05:59:41 (53.7 MB/s) - ‘datasets/vg/objects.json.zip’ saved [55323929/55323929]\n",
            "\n",
            "--2019-10-15 05:59:41--  https://visualgenome.org/static/data/dataset/attributes.json.zip\n",
            "Resolving visualgenome.org (visualgenome.org)... 54.87.32.87\n",
            "Connecting to visualgenome.org (visualgenome.org)|54.87.32.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83280561 (79M) [application/zip]\n",
            "Saving to: ‘datasets/vg/attributes.json.zip’\n",
            "\n",
            "datasets/vg/attribu 100%[===================>]  79.42M  65.2MB/s    in 1.2s    \n",
            "\n",
            "2019-10-15 05:59:43 (65.2 MB/s) - ‘datasets/vg/attributes.json.zip’ saved [83280561/83280561]\n",
            "\n",
            "--2019-10-15 05:59:43--  https://visualgenome.org/static/data/dataset/relationships.json.zip\n",
            "Resolving visualgenome.org (visualgenome.org)... 54.87.32.87\n",
            "Connecting to visualgenome.org (visualgenome.org)|54.87.32.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77904473 (74M) [application/zip]\n",
            "Saving to: ‘datasets/vg/relationships.json.zip’\n",
            "\n",
            "datasets/vg/relatio 100%[===================>]  74.29M  64.5MB/s    in 1.2s    \n",
            "\n",
            "2019-10-15 05:59:44 (64.5 MB/s) - ‘datasets/vg/relationships.json.zip’ saved [77904473/77904473]\n",
            "\n",
            "--2019-10-15 05:59:44--  https://visualgenome.org/static/data/dataset/object_alias.txt\n",
            "Resolving visualgenome.org (visualgenome.org)... 54.87.32.87\n",
            "Connecting to visualgenome.org (visualgenome.org)|54.87.32.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60166 (59K) [text/plain]\n",
            "Saving to: ‘datasets/vg/object_alias.txt’\n",
            "\n",
            "datasets/vg/object_ 100%[===================>]  58.76K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-10-15 05:59:44 (1.49 MB/s) - ‘datasets/vg/object_alias.txt’ saved [60166/60166]\n",
            "\n",
            "--2019-10-15 05:59:44--  https://visualgenome.org/static/data/dataset/relationship_alias.txt\n",
            "Resolving visualgenome.org (visualgenome.org)... 54.87.32.87\n",
            "Connecting to visualgenome.org (visualgenome.org)|54.87.32.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122102 (119K) [text/plain]\n",
            "Saving to: ‘datasets/vg/relationship_alias.txt’\n",
            "\n",
            "datasets/vg/relatio 100%[===================>] 119.24K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-10-15 05:59:44 (1.54 MB/s) - ‘datasets/vg/relationship_alias.txt’ saved [122102/122102]\n",
            "\n",
            "--2019-10-15 05:59:44--  https://visualgenome.org/static/data/dataset/image_data.json.zip\n",
            "Resolving visualgenome.org (visualgenome.org)... 54.87.32.87\n",
            "Connecting to visualgenome.org (visualgenome.org)|54.87.32.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1780854 (1.7M) [application/zip]\n",
            "Saving to: ‘datasets/vg/image_data.json.zip’\n",
            "\n",
            "datasets/vg/image_d 100%[===================>]   1.70M  5.54MB/s    in 0.3s    \n",
            "\n",
            "2019-10-15 05:59:45 (5.54 MB/s) - ‘datasets/vg/image_data.json.zip’ saved [1780854/1780854]\n",
            "\n",
            "--2019-10-15 05:59:45--  https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9731705982 (9.1G) [application/zip]\n",
            "Saving to: ‘datasets/vg/images.zip’\n",
            "\n",
            "datasets/vg/images.  94%[=================>  ]   8.58G  12.1MB/s    eta 37s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pERwK2AGy7cB",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ-s3BwYx18Z",
        "colab_type": "code",
        "outputId": "88fcc63b-d343-4951-da9a-a1cf9a8bdad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/python\n",
        "#\n",
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import argparse, json, os\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "from matplotlib.pyplot import imread\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "vocab for objects contains a special entry \"__image__\" intended to be used for\n",
        "dummy nodes encompassing the entire image; vocab for predicates includes a\n",
        "special entry \"__in_image__\" to be used for dummy relationships making the graph\n",
        "fully-connected.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "VG_DIR = 'datasets/vg'\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Input data\n",
        "parser.add_argument('--splits_json', default='sg2im/data/vg_splits.json')\n",
        "parser.add_argument('--images_json',\n",
        "    default=os.path.join(VG_DIR, 'image_data.json'))\n",
        "parser.add_argument('--objects_json',\n",
        "    default=os.path.join(VG_DIR, 'objects.json'))\n",
        "parser.add_argument('--attributes_json',\n",
        "    default=os.path.join(VG_DIR, 'attributes.json'))\n",
        "parser.add_argument('--object_aliases',\n",
        "    default=os.path.join(VG_DIR, 'object_alias.txt'))\n",
        "parser.add_argument('--relationship_aliases',\n",
        "    default=os.path.join(VG_DIR, 'relationship_alias.txt'))\n",
        "parser.add_argument('--relationships_json',\n",
        "    default=os.path.join(VG_DIR, 'relationships.json'))\n",
        "\n",
        "# Arguments for images\n",
        "parser.add_argument('--min_image_size', default=200, type=int)\n",
        "parser.add_argument('--train_split', default='train')\n",
        "\n",
        "# Arguments for objects\n",
        "parser.add_argument('--min_object_instances', default=2000, type=int)\n",
        "parser.add_argument('--min_attribute_instances', default=2000, type=int)\n",
        "parser.add_argument('--min_object_size', default=32, type=int)\n",
        "parser.add_argument('--min_objects_per_image', default=3, type=int)\n",
        "parser.add_argument('--max_objects_per_image', default=30, type=int)\n",
        "parser.add_argument('--max_attributes_per_image', default=30, type=int)\n",
        "\n",
        "# Arguments for relationships\n",
        "parser.add_argument('--min_relationship_instances', default=500, type=int)\n",
        "parser.add_argument('--min_relationships_per_image', default=1, type=int)\n",
        "parser.add_argument('--max_relationships_per_image', default=30, type=int)\n",
        "\n",
        "# Output\n",
        "parser.add_argument('--output_vocab_json',\n",
        "    default=os.path.join(VG_DIR, 'vocab.json'))\n",
        "parser.add_argument('--output_h5_dir', default=VG_DIR)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  print('Loading image info from \"%s\"' % args.images_json)\n",
        "  with open(args.images_json, 'r') as f:\n",
        "    images = json.load(f)\n",
        "  image_id_to_image = {i['image_id']: i for i in images}\n",
        "\n",
        "  with open(args.splits_json, 'r') as f:\n",
        "    splits = json.load(f)\n",
        "\n",
        "  # Filter images for being too small\n",
        "  splits = remove_small_images(args, image_id_to_image, splits)\n",
        "\n",
        "  obj_aliases = load_aliases(args.object_aliases)\n",
        "  rel_aliases = load_aliases(args.relationship_aliases)\n",
        "\n",
        "  print('Loading objects from \"%s\"' % args.objects_json)\n",
        "  with open(args.objects_json, 'r') as f:\n",
        "    objects = json.load(f)\n",
        "\n",
        "  # Vocab for objects and relationships\n",
        "  vocab = {}\n",
        "  train_ids = splits[args.train_split]\n",
        "  create_object_vocab(args, train_ids, objects, obj_aliases, vocab)\n",
        "\n",
        "  print('Loading attributes from \"%s\"' % args.attributes_json)\n",
        "  with open(args.attributes_json, 'r') as f:\n",
        "    attributes = json.load(f)\n",
        "\n",
        "  # Vocab for attributes\n",
        "  create_attribute_vocab(args, train_ids, attributes, vocab)\n",
        "\n",
        "  object_id_to_obj = filter_objects(args, objects, obj_aliases, vocab, splits)\n",
        "  print('After filtering there are %d object instances'\n",
        "        % len(object_id_to_obj))\n",
        "\n",
        "  print('Loading relationshps from \"%s\"' % args.relationships_json)\n",
        "  with open(args.relationships_json, 'r') as f:\n",
        "    relationships = json.load(f)\n",
        "\n",
        "  create_rel_vocab(args, train_ids, relationships, object_id_to_obj,\n",
        "                   rel_aliases, vocab)\n",
        "\n",
        "  print('Encoding objects and relationships ...')\n",
        "  numpy_arrays = encode_graphs(args, splits, objects, relationships, vocab,\n",
        "                               object_id_to_obj, attributes)\n",
        "\n",
        "  print('Writing HDF5 output files')\n",
        "  for split_name, split_arrays in numpy_arrays.items():\n",
        "    image_ids = list(split_arrays['image_ids'].astype(int))\n",
        "    h5_path = os.path.join(args.output_h5_dir, '%s.h5' % split_name)\n",
        "    print('Writing file \"%s\"' % h5_path)\n",
        "    with h5py.File(h5_path, 'w') as h5_file:\n",
        "      for name, ary in split_arrays.items():\n",
        "        print('Creating datset: ', name, ary.shape, ary.dtype)\n",
        "        h5_file.create_dataset(name, data=ary)\n",
        "      print('Writing image paths')\n",
        "      image_paths = get_image_paths(image_id_to_image, image_ids)\n",
        "      path_dtype = h5py.special_dtype(vlen=str)\n",
        "      path_shape = (len(image_paths),)\n",
        "      path_dset = h5_file.create_dataset('image_paths', path_shape,\n",
        "                                         dtype=path_dtype)\n",
        "      for i, p in enumerate(image_paths):\n",
        "        path_dset[i] = p\n",
        "    print()\n",
        "\n",
        "  print('Writing vocab to \"%s\"' % args.output_vocab_json)\n",
        "  with open(args.output_vocab_json, 'w') as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "def remove_small_images(args, image_id_to_image, splits):\n",
        "  new_splits = {}\n",
        "  for split_name, image_ids in splits.items():\n",
        "    new_image_ids = []\n",
        "    num_skipped = 0\n",
        "    for image_id in image_ids:\n",
        "      image = image_id_to_image[image_id]\n",
        "      height, width = image['height'], image['width']\n",
        "      if min(height, width) < args.min_image_size:\n",
        "        num_skipped += 1\n",
        "        continue\n",
        "      new_image_ids.append(image_id)\n",
        "    new_splits[split_name] = new_image_ids\n",
        "    print('Removed %d images from split \"%s\" for being too small' %\n",
        "          (num_skipped, split_name))\n",
        "\n",
        "  return new_splits\n",
        "\n",
        "\n",
        "def get_image_paths(image_id_to_image, image_ids):\n",
        "  paths = []\n",
        "  for image_id in image_ids:\n",
        "    image = image_id_to_image[image_id]\n",
        "    base, filename = os.path.split(image['url'])\n",
        "    path = os.path.join(os.path.basename(base), filename)\n",
        "    paths.append(path)\n",
        "  return paths\n",
        "\n",
        "\n",
        "def handle_images(args, image_ids, h5_file):\n",
        "  with open(args.images_json, 'r') as f:\n",
        "    images = json.load(f)\n",
        "  if image_ids:\n",
        "    image_ids = set(image_ids)\n",
        "\n",
        "  image_heights, image_widths = [], []\n",
        "  image_ids_out, image_paths = [], []\n",
        "  for image in images:\n",
        "    image_id = image['image_id']\n",
        "    if image_ids and image_id not in image_ids:\n",
        "      continue\n",
        "    height, width = image['height'], image['width']\n",
        "\n",
        "    base, filename = os.path.split(image['url'])\n",
        "    path = os.path.join(os.path.basename(base), filename)\n",
        "    image_paths.append(path)\n",
        "    image_heights.append(height)\n",
        "    image_widths.append(width)\n",
        "    image_ids_out.append(image_id)\n",
        "\n",
        "  image_ids_np = np.asarray(image_ids_out, dtype=int)\n",
        "  h5_file.create_dataset('image_ids', data=image_ids_np)\n",
        "\n",
        "  image_heights = np.asarray(image_heights, dtype=int)\n",
        "  h5_file.create_dataset('image_heights', data=image_heights)\n",
        "\n",
        "  image_widths = np.asarray(image_widths, dtype=int)\n",
        "  h5_file.create_dataset('image_widths', data=image_widths)\n",
        "\n",
        "  return image_paths\n",
        "\n",
        "\n",
        "def load_aliases(alias_path):\n",
        "  aliases = {}\n",
        "  print('Loading aliases from \"%s\"' % alias_path)\n",
        "  with open(alias_path, 'r') as f:\n",
        "    for line in f:\n",
        "      line = [s.strip() for s in line.split(',')]\n",
        "      for s in line:\n",
        "        aliases[s] = line[0]\n",
        "  return aliases\n",
        "\n",
        "\n",
        "def create_object_vocab(args, image_ids, objects, aliases, vocab):\n",
        "  image_ids = set(image_ids)\n",
        "\n",
        "  print('Making object vocab from %d training images' % len(image_ids))\n",
        "  object_name_counter = Counter()\n",
        "  for image in objects:\n",
        "    if image['image_id'] not in image_ids:\n",
        "      continue\n",
        "    for obj in image['objects']:\n",
        "      names = set()\n",
        "      for name in obj['names']:\n",
        "        names.add(aliases.get(name, name))\n",
        "      object_name_counter.update(names)\n",
        "\n",
        "  object_names = ['__image__']\n",
        "  for name, count in object_name_counter.most_common():\n",
        "    if count >= args.min_object_instances:\n",
        "      object_names.append(name)\n",
        "  print('Found %d object categories with >= %d training instances' %\n",
        "        (len(object_names), args.min_object_instances))\n",
        "\n",
        "  object_name_to_idx = {}\n",
        "  object_idx_to_name = []\n",
        "  for idx, name in enumerate(object_names):\n",
        "    object_name_to_idx[name] = idx\n",
        "    object_idx_to_name.append(name)\n",
        "\n",
        "  vocab['object_name_to_idx'] = object_name_to_idx\n",
        "  vocab['object_idx_to_name'] = object_idx_to_name\n",
        "\n",
        "def create_attribute_vocab(args, image_ids, attributes, vocab):\n",
        "  image_ids = set(image_ids)\n",
        "  print('Making attribute vocab from %d training images' % len(image_ids))\n",
        "  attribute_name_counter = Counter()\n",
        "  for image in attributes:\n",
        "    if image['image_id'] not in image_ids:\n",
        "      continue\n",
        "    for attribute in image['attributes']:\n",
        "      names = set()\n",
        "      try:\n",
        "        for name in attribute['attributes']:\n",
        "          names.add(name)\n",
        "        attribute_name_counter.update(names)\n",
        "      except KeyError:\n",
        "        pass\n",
        "  attribute_names = []\n",
        "  for name, count in attribute_name_counter.most_common():\n",
        "    if count >= args.min_attribute_instances:\n",
        "      attribute_names.append(name)\n",
        "  print('Found %d attribute categories with >= %d training instances' %\n",
        "        (len(attribute_names), args.min_attribute_instances))\n",
        "\n",
        "  attribute_name_to_idx = {}\n",
        "  attribute_idx_to_name = []\n",
        "  for idx, name in enumerate(attribute_names):\n",
        "    attribute_name_to_idx[name] = idx\n",
        "    attribute_idx_to_name.append(name)\n",
        "  vocab['attribute_name_to_idx'] = attribute_name_to_idx\n",
        "  vocab['attribute_idx_to_name'] = attribute_idx_to_name\n",
        "\n",
        "def filter_objects(args, objects, aliases, vocab, splits):\n",
        "  object_id_to_objects = {}\n",
        "  all_image_ids = set()\n",
        "  for image_ids in splits.values():\n",
        "    all_image_ids |= set(image_ids)\n",
        "\n",
        "  object_name_to_idx = vocab['object_name_to_idx']\n",
        "  object_id_to_obj = {}\n",
        "\n",
        "  num_too_small = 0\n",
        "  for image in objects:\n",
        "    image_id = image['image_id']\n",
        "    if image_id not in all_image_ids:\n",
        "      continue\n",
        "    for obj in image['objects']:\n",
        "      object_id = obj['object_id']\n",
        "      final_name = None\n",
        "      final_name_idx = None\n",
        "      for name in obj['names']:\n",
        "        name = aliases.get(name, name)\n",
        "        if name in object_name_to_idx:\n",
        "          final_name = name\n",
        "          final_name_idx = object_name_to_idx[final_name]\n",
        "          break\n",
        "      w, h = obj['w'], obj['h']\n",
        "      too_small = (w < args.min_object_size) or (h < args.min_object_size)\n",
        "      if too_small:\n",
        "        num_too_small += 1\n",
        "      if final_name is not None and not too_small:\n",
        "        object_id_to_obj[object_id] = {\n",
        "          'name': final_name,\n",
        "          'name_idx': final_name_idx,\n",
        "          'box': [obj['x'], obj['y'], obj['w'], obj['h']],\n",
        "        }\n",
        "  print('Skipped %d objects with size < %d' % (num_too_small, args.min_object_size))\n",
        "  return object_id_to_obj\n",
        "\n",
        "\n",
        "def create_rel_vocab(args, image_ids, relationships, object_id_to_obj,\n",
        "                     rel_aliases, vocab):\n",
        "  pred_counter = defaultdict(int)\n",
        "  image_ids_set = set(image_ids)\n",
        "  for image in relationships:\n",
        "    image_id = image['image_id']\n",
        "    if image_id not in image_ids_set:\n",
        "      continue\n",
        "    for rel in image['relationships']:\n",
        "      sid = rel['subject']['object_id']\n",
        "      oid = rel['object']['object_id']\n",
        "      found_subject = sid in object_id_to_obj\n",
        "      found_object = oid in object_id_to_obj\n",
        "      if not found_subject or not found_object:\n",
        "        continue\n",
        "      pred = rel['predicate'].lower().strip()\n",
        "      pred = rel_aliases.get(pred, pred)\n",
        "      rel['predicate'] = pred\n",
        "      pred_counter[pred] += 1\n",
        "\n",
        "  pred_names = ['__in_image__']\n",
        "  for pred, count in pred_counter.items():\n",
        "    if count >= args.min_relationship_instances:\n",
        "      pred_names.append(pred)\n",
        "  print('Found %d relationship types with >= %d training instances'\n",
        "        % (len(pred_names), args.min_relationship_instances))\n",
        "\n",
        "  pred_name_to_idx = {}\n",
        "  pred_idx_to_name = []\n",
        "  for idx, name in enumerate(pred_names):\n",
        "    pred_name_to_idx[name] = idx\n",
        "    pred_idx_to_name.append(name)\n",
        "\n",
        "  vocab['pred_name_to_idx'] = pred_name_to_idx\n",
        "  vocab['pred_idx_to_name'] = pred_idx_to_name\n",
        "\n",
        "\n",
        "def encode_graphs(args, splits, objects, relationships, vocab,\n",
        "                  object_id_to_obj, attributes):\n",
        "\n",
        "  image_id_to_objects = {}\n",
        "  for image in objects:\n",
        "    image_id = image['image_id']\n",
        "    image_id_to_objects[image_id] = image['objects']\n",
        "  image_id_to_relationships = {}\n",
        "  for image in relationships:\n",
        "    image_id = image['image_id']\n",
        "    image_id_to_relationships[image_id] = image['relationships']\n",
        "  image_id_to_attributes = {}\n",
        "  for image in attributes:\n",
        "    image_id = image['image_id']\n",
        "    image_id_to_attributes[image_id] = image['attributes']\n",
        "\n",
        "  numpy_arrays = {}\n",
        "  for split, image_ids in splits.items():\n",
        "    skip_stats = defaultdict(int)\n",
        "    # We need to filter *again* based on number of objects and relationships\n",
        "    final_image_ids = []\n",
        "    object_ids = []\n",
        "    object_names = []\n",
        "    object_boxes = []\n",
        "    objects_per_image = []\n",
        "    relationship_ids = []\n",
        "    relationship_subjects = []\n",
        "    relationship_predicates = []\n",
        "    relationship_objects = []\n",
        "    relationships_per_image = []\n",
        "    attribute_ids = []\n",
        "    attributes_per_object = []\n",
        "    object_attributes = []\n",
        "    for image_id in image_ids:\n",
        "      image_object_ids = []\n",
        "      image_object_names = []\n",
        "      image_object_boxes = []\n",
        "      object_id_to_idx = {}\n",
        "      for obj in image_id_to_objects[image_id]:\n",
        "        object_id = obj['object_id']\n",
        "        if object_id not in object_id_to_obj:\n",
        "          continue\n",
        "        obj = object_id_to_obj[object_id]\n",
        "        object_id_to_idx[object_id] = len(image_object_ids)\n",
        "        image_object_ids.append(object_id)\n",
        "        image_object_names.append(obj['name_idx'])\n",
        "        image_object_boxes.append(obj['box'])\n",
        "      num_objects = len(image_object_ids)\n",
        "      too_few = num_objects < args.min_objects_per_image\n",
        "      too_many = num_objects > args.max_objects_per_image\n",
        "      if too_few:\n",
        "        skip_stats['too_few_objects'] += 1\n",
        "        continue\n",
        "      if too_many:\n",
        "        skip_stats['too_many_objects'] += 1\n",
        "        continue\n",
        "      image_rel_ids = []\n",
        "      image_rel_subs = []\n",
        "      image_rel_preds = []\n",
        "      image_rel_objs = []\n",
        "      for rel in image_id_to_relationships[image_id]:\n",
        "        relationship_id = rel['relationship_id']\n",
        "        pred = rel['predicate']\n",
        "        pred_idx = vocab['pred_name_to_idx'].get(pred, None)\n",
        "        if pred_idx is None:\n",
        "          continue\n",
        "        sid = rel['subject']['object_id']\n",
        "        sidx = object_id_to_idx.get(sid, None)\n",
        "        oid = rel['object']['object_id']\n",
        "        oidx = object_id_to_idx.get(oid, None)\n",
        "        if sidx is None or oidx is None:\n",
        "          continue\n",
        "        image_rel_ids.append(relationship_id)\n",
        "        image_rel_subs.append(sidx)\n",
        "        image_rel_preds.append(pred_idx)\n",
        "        image_rel_objs.append(oidx)\n",
        "      num_relationships = len(image_rel_ids)\n",
        "      too_few = num_relationships < args.min_relationships_per_image\n",
        "      too_many = num_relationships > args.max_relationships_per_image\n",
        "      if too_few:\n",
        "        skip_stats['too_few_relationships'] += 1\n",
        "        continue\n",
        "      if too_many:\n",
        "        skip_stats['too_many_relationships'] += 1\n",
        "        continue\n",
        "\n",
        "      obj_id_to_attributes = {}\n",
        "      num_attributes = []\n",
        "      for obj_attribute in image_id_to_attributes[image_id]:\n",
        "        obj_id_to_attributes[obj_attribute['object_id']] = obj_attribute.get('attributes', None)\n",
        "      for object_id in image_object_ids:\n",
        "        attributes = obj_id_to_attributes.get(object_id, None)\n",
        "        if attributes is None:\n",
        "          object_attributes.append([-1] * args.max_attributes_per_image)\n",
        "          num_attributes.append(0)\n",
        "        else:\n",
        "          attribute_ids = []\n",
        "          for attribute in attributes:\n",
        "            if attribute in vocab['attribute_name_to_idx']:\n",
        "              attribute_ids.append(vocab['attribute_name_to_idx'][attribute])\n",
        "            if len(attribute_ids) >= args.max_attributes_per_image:\n",
        "              break\n",
        "          num_attributes.append(len(attribute_ids))\n",
        "          pad_len = args.max_attributes_per_image - len(attribute_ids)\n",
        "          attribute_ids = attribute_ids + [-1] * pad_len\n",
        "          object_attributes.append(attribute_ids)\n",
        "\n",
        "      # Pad object info out to max_objects_per_image\n",
        "      while len(image_object_ids) < args.max_objects_per_image:\n",
        "        image_object_ids.append(-1)\n",
        "        image_object_names.append(-1)\n",
        "        image_object_boxes.append([-1, -1, -1, -1])\n",
        "        num_attributes.append(-1)\n",
        "\n",
        "      # Pad relationship info out to max_relationships_per_image\n",
        "      while len(image_rel_ids) < args.max_relationships_per_image:\n",
        "        image_rel_ids.append(-1)\n",
        "        image_rel_subs.append(-1)\n",
        "        image_rel_preds.append(-1)\n",
        "        image_rel_objs.append(-1)\n",
        "\n",
        "      final_image_ids.append(image_id)\n",
        "      object_ids.append(image_object_ids)\n",
        "      object_names.append(image_object_names)\n",
        "      object_boxes.append(image_object_boxes)\n",
        "      objects_per_image.append(num_objects)\n",
        "      relationship_ids.append(image_rel_ids)\n",
        "      relationship_subjects.append(image_rel_subs)\n",
        "      relationship_predicates.append(image_rel_preds)\n",
        "      relationship_objects.append(image_rel_objs)\n",
        "      relationships_per_image.append(num_relationships)\n",
        "      attributes_per_object.append(num_attributes)\n",
        "\n",
        "    print('Skip stats for split \"%s\"' % split)\n",
        "    for stat, count in skip_stats.items():\n",
        "      print(stat, count)\n",
        "    print()\n",
        "    numpy_arrays[split] = {\n",
        "      'image_ids': np.asarray(final_image_ids),\n",
        "      'object_ids': np.asarray(object_ids),\n",
        "      'object_names': np.asarray(object_names),\n",
        "      'object_boxes': np.asarray(object_boxes),\n",
        "      'objects_per_image': np.asarray(objects_per_image),\n",
        "      'relationship_ids': np.asarray(relationship_ids),\n",
        "      'relationship_subjects': np.asarray(relationship_subjects),\n",
        "      'relationship_predicates': np.asarray(relationship_predicates),\n",
        "      'relationship_objects': np.asarray(relationship_objects),\n",
        "      'relationships_per_image': np.asarray(relationships_per_image),\n",
        "      'attributes_per_object': np.asarray(attributes_per_object),\n",
        "      'object_attributes': np.asarray(object_attributes),\n",
        "    }\n",
        "    for k, v in numpy_arrays[split].items():\n",
        "      if v.dtype == np.int64:\n",
        "        numpy_arrays[split][k] = v.astype(np.int32)\n",
        "  return numpy_arrays\n",
        "\n",
        "import sys\n",
        "sys.argv=[sys.argv[0]]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  args = parser.parse_args()\n",
        "  main(args)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading image info from \"datasets/vg/image_data.json\"\n",
            "Removed 335 images from split \"train\" for being too small\n",
            "Removed 45 images from split \"test\" for being too small\n",
            "Removed 46 images from split \"val\" for being too small\n",
            "Loading aliases from \"datasets/vg/object_alias.txt\"\n",
            "Loading aliases from \"datasets/vg/relationship_alias.txt\"\n",
            "Loading objects from \"datasets/vg/objects.json\"\n",
            "Making object vocab from 86128 training images\n",
            "Found 179 object categories with >= 2000 training instances\n",
            "Loading attributes from \"datasets/vg/attributes.json\"\n",
            "Making attribute vocab from 86128 training images\n",
            "Found 80 attribute categories with >= 2000 training instances\n",
            "Skipped 997213 objects with size < 32\n",
            "After filtering there are 910259 object instances\n",
            "Loading relationshps from \"datasets/vg/relationships.json\"\n",
            "Found 46 relationship types with >= 500 training instances\n",
            "Encoding objects and relationships ...\n",
            "Skip stats for split \"train\"\n",
            "too_few_relationships 16402\n",
            "too_few_objects 6794\n",
            "too_many_objects 187\n",
            "too_many_relationships 180\n",
            "\n",
            "Skip stats for split \"test\"\n",
            "too_few_relationships 4803\n",
            "too_few_objects 837\n",
            "too_many_objects 26\n",
            "\n",
            "Skip stats for split \"val\"\n",
            "too_few_objects 853\n",
            "too_few_relationships 4815\n",
            "too_many_objects 27\n",
            "too_many_relationships 4\n",
            "\n",
            "Writing HDF5 output files\n",
            "Writing file \"datasets/vg/train.h5\"\n",
            "Creating datset:  image_ids (62565,) int32\n",
            "Creating datset:  object_ids (62565, 30) int32\n",
            "Creating datset:  object_names (62565, 30) int32\n",
            "Creating datset:  object_boxes (62565, 30, 4) int32\n",
            "Creating datset:  objects_per_image (62565,) int32\n",
            "Creating datset:  relationship_ids (62565, 30) int32\n",
            "Creating datset:  relationship_subjects (62565, 30) int32\n",
            "Creating datset:  relationship_predicates (62565, 30) int32\n",
            "Creating datset:  relationship_objects (62565, 30) int32\n",
            "Creating datset:  relationships_per_image (62565,) int32\n",
            "Creating datset:  attributes_per_object (62565, 30) int32\n",
            "Creating datset:  object_attributes (606319, 30) int32\n",
            "Writing image paths\n",
            "\n",
            "Writing file \"datasets/vg/test.h5\"\n",
            "Creating datset:  image_ids (5096,) int32\n",
            "Creating datset:  object_ids (5096, 30) int32\n",
            "Creating datset:  object_names (5096, 30) int32\n",
            "Creating datset:  object_boxes (5096, 30, 4) int32\n",
            "Creating datset:  objects_per_image (5096,) int32\n",
            "Creating datset:  relationship_ids (5096, 30) int32\n",
            "Creating datset:  relationship_subjects (5096, 30) int32\n",
            "Creating datset:  relationship_predicates (5096, 30) int32\n",
            "Creating datset:  relationship_objects (5096, 30) int32\n",
            "Creating datset:  relationships_per_image (5096,) int32\n",
            "Creating datset:  attributes_per_object (5096, 30) int32\n",
            "Creating datset:  object_attributes (51626, 30) int32\n",
            "Writing image paths\n",
            "\n",
            "Writing file \"datasets/vg/val.h5\"\n",
            "Creating datset:  image_ids (5062,) int32\n",
            "Creating datset:  object_ids (5062, 30) int32\n",
            "Creating datset:  object_names (5062, 30) int32\n",
            "Creating datset:  object_boxes (5062, 30, 4) int32\n",
            "Creating datset:  objects_per_image (5062,) int32\n",
            "Creating datset:  relationship_ids (5062, 30) int32\n",
            "Creating datset:  relationship_subjects (5062, 30) int32\n",
            "Creating datset:  relationship_predicates (5062, 30) int32\n",
            "Creating datset:  relationship_objects (5062, 30) int32\n",
            "Creating datset:  relationships_per_image (5062,) int32\n",
            "Creating datset:  attributes_per_object (5062, 30) int32\n",
            "Creating datset:  object_attributes (51090, 30) int32\n",
            "Writing image paths\n",
            "\n",
            "Writing vocab to \"datasets/vg/vocab.json\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeJuZh0lzH6f",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rTdm_0nk4AaR",
        "outputId": "604754fa-eb2e-49bd-a655-e2ce4325cc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/python\n",
        "#\n",
        "# Copyright 2018 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import argparse\n",
        "import functools\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sg2im.data import imagenet_deprocess_batch\n",
        "from sg2im.data.coco import CocoSceneGraphDataset, coco_collate_fn\n",
        "from sg2im.data.vg import VgSceneGraphDataset, vg_collate_fn\n",
        "from sg2im.discriminators import PatchDiscriminator, AcCropDiscriminator\n",
        "from sg2im.losses import get_gan_losses\n",
        "from sg2im.metrics import jaccard\n",
        "from sg2im.model import Sg2ImModel\n",
        "from sg2im.utils import int_tuple, float_tuple, str_tuple\n",
        "from sg2im.utils import timeit, bool_flag, LossManager\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "VG_DIR = os.path.expanduser('datasets/vg')\n",
        "COCO_DIR = os.path.expanduser('datasets/coco')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', default='coco', choices=['vg', 'coco'])\n",
        "\n",
        "# Optimization hyperparameters\n",
        "parser.add_argument('--batch_size', default=32, type=int)\n",
        "parser.add_argument('--num_iterations', default=1000000, type=int)\n",
        "parser.add_argument('--learning_rate', default=1e-4, type=float)\n",
        "\n",
        "# Switch the generator to eval mode after this many iterations\n",
        "parser.add_argument('--eval_mode_after', default=100000, type=int)\n",
        "\n",
        "# Dataset options common to both VG and COCO\n",
        "parser.add_argument('--image_size', default='64,64', type=int_tuple)\n",
        "parser.add_argument('--num_train_samples', default=None, type=int)\n",
        "parser.add_argument('--num_val_samples', default=1024, type=int)\n",
        "parser.add_argument('--shuffle_val', default=True, type=bool_flag)\n",
        "parser.add_argument('--loader_num_workers', default=4, type=int)\n",
        "parser.add_argument('--include_relationships', default=True, type=bool_flag)\n",
        "\n",
        "# VG-specific options\n",
        "parser.add_argument('--vg_image_dir', default=os.path.join(VG_DIR, 'images'))\n",
        "parser.add_argument('--train_h5', default=os.path.join(VG_DIR, 'train.h5'))\n",
        "parser.add_argument('--val_h5', default=os.path.join(VG_DIR, 'val.h5'))\n",
        "parser.add_argument('--vocab_json', default=os.path.join(VG_DIR, 'vocab.json'))\n",
        "parser.add_argument('--max_objects_per_image', default=10, type=int)\n",
        "parser.add_argument('--vg_use_orphaned_objects', default=True, type=bool_flag)\n",
        "\n",
        "# COCO-specific options\n",
        "parser.add_argument('--coco_train_image_dir',\n",
        "         default=os.path.join(COCO_DIR, 'images/train2017'))\n",
        "parser.add_argument('--coco_val_image_dir',\n",
        "         default=os.path.join(COCO_DIR, 'images/val2017'))\n",
        "parser.add_argument('--coco_train_instances_json',\n",
        "         default=os.path.join(COCO_DIR, 'annotations/instances_train2017.json'))\n",
        "parser.add_argument('--coco_train_stuff_json',\n",
        "         default=os.path.join(COCO_DIR, 'annotations/stuff_train2017.json'))\n",
        "parser.add_argument('--coco_val_instances_json',\n",
        "         default=os.path.join(COCO_DIR, 'annotations/instances_val2017.json'))\n",
        "parser.add_argument('--coco_val_stuff_json',\n",
        "         default=os.path.join(COCO_DIR, 'annotations/stuff_val2017.json'))\n",
        "parser.add_argument('--instance_whitelist', default=None, type=str_tuple)\n",
        "parser.add_argument('--stuff_whitelist', default=None, type=str_tuple)\n",
        "parser.add_argument('--coco_include_other', default=False, type=bool_flag)\n",
        "parser.add_argument('--min_object_size', default=0.02, type=float)\n",
        "parser.add_argument('--min_objects_per_image', default=3, type=int)\n",
        "parser.add_argument('--coco_stuff_only', default=True, type=bool_flag)\n",
        "\n",
        "# Generator options\n",
        "parser.add_argument('--mask_size', default=16, type=int) # Set this to 0 to use no masks\n",
        "parser.add_argument('--embedding_dim', default=128, type=int)\n",
        "parser.add_argument('--gconv_dim', default=128, type=int)\n",
        "parser.add_argument('--gconv_hidden_dim', default=512, type=int)\n",
        "parser.add_argument('--gconv_num_layers', default=5, type=int)\n",
        "parser.add_argument('--mlp_normalization', default='none', type=str)\n",
        "parser.add_argument('--refinement_network_dims', default='1024,512,256,128,64', type=int_tuple)\n",
        "parser.add_argument('--normalization', default='batch')\n",
        "parser.add_argument('--activation', default='leakyrelu-0.2')\n",
        "parser.add_argument('--layout_noise_dim', default=32, type=int)\n",
        "parser.add_argument('--use_boxes_pred_after', default=-1, type=int)\n",
        "\n",
        "# Generator losses\n",
        "parser.add_argument('--mask_loss_weight', default=0, type=float)\n",
        "parser.add_argument('--l1_pixel_loss_weight', default=1.0, type=float)\n",
        "parser.add_argument('--bbox_pred_loss_weight', default=10, type=float)\n",
        "parser.add_argument('--predicate_pred_loss_weight', default=0, type=float) # DEPRECATED\n",
        "\n",
        "# Generic discriminator options\n",
        "parser.add_argument('--discriminator_loss_weight', default=0.01, type=float)\n",
        "parser.add_argument('--gan_loss_type', default='gan')\n",
        "parser.add_argument('--d_clip', default=None, type=float)\n",
        "parser.add_argument('--d_normalization', default='batch')\n",
        "parser.add_argument('--d_padding', default='valid')\n",
        "parser.add_argument('--d_activation', default='leakyrelu-0.2')\n",
        "\n",
        "# Object discriminator\n",
        "parser.add_argument('--d_obj_arch',\n",
        "    default='C4-64-2,C4-128-2,C4-256-2')\n",
        "parser.add_argument('--crop_size', default=32, type=int)\n",
        "parser.add_argument('--d_obj_weight', default=1.0, type=float) # multiplied by d_loss_weight \n",
        "parser.add_argument('--ac_loss_weight', default=0.1, type=float)\n",
        "\n",
        "# Image discriminator\n",
        "parser.add_argument('--d_img_arch',\n",
        "    default='C4-64-2,C4-128-2,C4-256-2')\n",
        "parser.add_argument('--d_img_weight', default=1.0, type=float) # multiplied by d_loss_weight\n",
        "\n",
        "# Output options\n",
        "parser.add_argument('--print_every', default=10, type=int)\n",
        "parser.add_argument('--timing', default=False, type=bool_flag)\n",
        "parser.add_argument('--checkpoint_every', default=10000, type=int)\n",
        "parser.add_argument('--output_dir', default=os.getcwd())\n",
        "parser.add_argument('--checkpoint_name', default='checkpoint')\n",
        "parser.add_argument('--checkpoint_start_from', default=None)\n",
        "parser.add_argument('--restore_from_checkpoint', default=False, type=bool_flag)\n",
        "\n",
        "\n",
        "def add_loss(total_loss, curr_loss, loss_dict, loss_name, weight=1):\n",
        "  curr_loss = curr_loss * weight\n",
        "  loss_dict[loss_name] = curr_loss.item()\n",
        "  if total_loss is not None:\n",
        "    total_loss += curr_loss\n",
        "  else:\n",
        "    total_loss = curr_loss\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def check_args(args):\n",
        "  H, W = args.image_size\n",
        "  for _ in args.refinement_network_dims[1:]:\n",
        "    H = H // 2\n",
        "  if H == 0:\n",
        "    raise ValueError(\"Too many layers in refinement network\")\n",
        "\n",
        "\n",
        "def build_model(args, vocab):\n",
        "  if args.checkpoint_start_from is not None:\n",
        "    checkpoint = torch.load(args.checkpoint_start_from)\n",
        "    kwargs = checkpoint['model_kwargs']\n",
        "    model = Sg2ImModel(**kwargs)\n",
        "    raw_state_dict = checkpoint['model_state']\n",
        "    state_dict = {}\n",
        "    for k, v in raw_state_dict.items():\n",
        "      if k.startswith('module.'):\n",
        "        k = k[7:]\n",
        "      state_dict[k] = v\n",
        "    model.load_state_dict(state_dict)\n",
        "  else:\n",
        "    kwargs = {\n",
        "      'vocab': vocab,\n",
        "      'image_size': args.image_size,\n",
        "      'embedding_dim': args.embedding_dim,\n",
        "      'gconv_dim': args.gconv_dim,\n",
        "      'gconv_hidden_dim': args.gconv_hidden_dim,\n",
        "      'gconv_num_layers': args.gconv_num_layers,\n",
        "      'mlp_normalization': args.mlp_normalization,\n",
        "      'refinement_dims': args.refinement_network_dims,\n",
        "      'normalization': args.normalization,\n",
        "      'activation': args.activation,\n",
        "      'mask_size': args.mask_size,\n",
        "      'layout_noise_dim': args.layout_noise_dim,\n",
        "    }\n",
        "    model = Sg2ImModel(**kwargs)\n",
        "  return model, kwargs\n",
        "\n",
        "\n",
        "def build_obj_discriminator(args, vocab):\n",
        "  discriminator = None\n",
        "  d_kwargs = {}\n",
        "  d_weight = args.discriminator_loss_weight\n",
        "  d_obj_weight = args.d_obj_weight\n",
        "  if d_weight == 0 or d_obj_weight == 0:\n",
        "    return discriminator, d_kwargs\n",
        "\n",
        "  d_kwargs = {\n",
        "    'vocab': vocab,\n",
        "    'arch': args.d_obj_arch,\n",
        "    'normalization': args.d_normalization,\n",
        "    'activation': args.d_activation,\n",
        "    'padding': args.d_padding,\n",
        "    'object_size': args.crop_size,\n",
        "  }\n",
        "  discriminator = AcCropDiscriminator(**d_kwargs)\n",
        "  return discriminator, d_kwargs\n",
        "\n",
        "\n",
        "def build_img_discriminator(args, vocab):\n",
        "  discriminator = None\n",
        "  d_kwargs = {}\n",
        "  d_weight = args.discriminator_loss_weight\n",
        "  d_img_weight = args.d_img_weight\n",
        "  if d_weight == 0 or d_img_weight == 0:\n",
        "    return discriminator, d_kwargs\n",
        "\n",
        "  d_kwargs = {\n",
        "    'arch': args.d_img_arch,\n",
        "    'normalization': args.d_normalization,\n",
        "    'activation': args.d_activation,\n",
        "    'padding': args.d_padding,\n",
        "  }\n",
        "  discriminator = PatchDiscriminator(**d_kwargs)\n",
        "  return discriminator, d_kwargs\n",
        "\n",
        "\n",
        "def build_coco_dsets(args):\n",
        "  dset_kwargs = {\n",
        "    'image_dir': args.coco_train_image_dir,\n",
        "    'instances_json': args.coco_train_instances_json,\n",
        "    'stuff_json': args.coco_train_stuff_json,\n",
        "    'stuff_only': args.coco_stuff_only,\n",
        "    'image_size': args.image_size,\n",
        "    'mask_size': args.mask_size,\n",
        "    'max_samples': args.num_train_samples,\n",
        "    'min_object_size': args.min_object_size,\n",
        "    'min_objects_per_image': args.min_objects_per_image,\n",
        "    'instance_whitelist': args.instance_whitelist,\n",
        "    'stuff_whitelist': args.stuff_whitelist,\n",
        "    'include_other': args.coco_include_other,\n",
        "    'include_relationships': args.include_relationships,\n",
        "  }\n",
        "  train_dset = CocoSceneGraphDataset(**dset_kwargs)\n",
        "  num_objs = train_dset.total_objects()\n",
        "  num_imgs = len(train_dset)\n",
        "  print('Training dataset has %d images and %d objects' % (num_imgs, num_objs))\n",
        "  print('(%.2f objects per image)' % (float(num_objs) / num_imgs))\n",
        "\n",
        "  dset_kwargs['image_dir'] = args.coco_val_image_dir\n",
        "  dset_kwargs['instances_json'] = args.coco_val_instances_json\n",
        "  dset_kwargs['stuff_json'] = args.coco_val_stuff_json\n",
        "  dset_kwargs['max_samples'] = args.num_val_samples\n",
        "  val_dset = CocoSceneGraphDataset(**dset_kwargs)\n",
        "\n",
        "  assert train_dset.vocab == val_dset.vocab\n",
        "  vocab = json.loads(json.dumps(train_dset.vocab))\n",
        "\n",
        "  return vocab, train_dset, val_dset\n",
        "\n",
        "\n",
        "def build_vg_dsets(args):\n",
        "  with open(args.vocab_json, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "  dset_kwargs = {\n",
        "    'vocab': vocab,\n",
        "    'h5_path': args.train_h5,\n",
        "    'image_dir': args.vg_image_dir,\n",
        "    'image_size': args.image_size,\n",
        "    'max_samples': args.num_train_samples,\n",
        "    'max_objects': args.max_objects_per_image,\n",
        "    'use_orphaned_objects': args.vg_use_orphaned_objects,\n",
        "    'include_relationships': args.include_relationships,\n",
        "  }\n",
        "  train_dset = VgSceneGraphDataset(**dset_kwargs)\n",
        "  iter_per_epoch = len(train_dset) // args.batch_size\n",
        "  print('There are %d iterations per epoch' % iter_per_epoch)\n",
        "\n",
        "  dset_kwargs['h5_path'] = args.val_h5\n",
        "  del dset_kwargs['max_samples']\n",
        "  val_dset = VgSceneGraphDataset(**dset_kwargs)\n",
        "  \n",
        "  return vocab, train_dset, val_dset\n",
        "\n",
        "\n",
        "def build_loaders(args):\n",
        "  if args.dataset == 'vg':\n",
        "    vocab, train_dset, val_dset = build_vg_dsets(args)\n",
        "    collate_fn = vg_collate_fn\n",
        "  elif args.dataset == 'coco':\n",
        "    vocab, train_dset, val_dset = build_coco_dsets(args)\n",
        "    collate_fn = coco_collate_fn\n",
        "\n",
        "  loader_kwargs = {\n",
        "    'batch_size': args.batch_size,\n",
        "    'num_workers': args.loader_num_workers,\n",
        "    'shuffle': True,\n",
        "    'collate_fn': collate_fn,\n",
        "  }\n",
        "  train_loader = DataLoader(train_dset, **loader_kwargs)\n",
        "  \n",
        "  loader_kwargs['shuffle'] = args.shuffle_val\n",
        "  val_loader = DataLoader(val_dset, **loader_kwargs)\n",
        "  return vocab, train_loader, val_loader\n",
        "\n",
        "\n",
        "def check_model(args, t, loader, model):\n",
        "  float_dtype = torch.cuda.FloatTensor\n",
        "  long_dtype = torch.cuda.LongTensor\n",
        "  num_samples = 0\n",
        "  all_losses = defaultdict(list)\n",
        "  total_iou = 0\n",
        "  total_boxes = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in loader:\n",
        "      batch = [tensor.cuda() for tensor in batch]\n",
        "      masks = None\n",
        "      if len(batch) == 6:\n",
        "        imgs, objs, boxes, triples, obj_to_img, triple_to_img = batch\n",
        "      elif len(batch) == 7:\n",
        "        imgs, objs, boxes, masks, triples, obj_to_img, triple_to_img = batch\n",
        "      predicates = triples[:, 1] \n",
        "\n",
        "      # Run the model as it has been run during training\n",
        "      model_masks = masks\n",
        "      model_out = model(objs, triples, obj_to_img, boxes_gt=boxes, masks_gt=model_masks)\n",
        "      imgs_pred, boxes_pred, masks_pred, predicate_scores = model_out\n",
        "\n",
        "      skip_pixel_loss = False\n",
        "      total_loss, losses =  calculate_model_losses(\n",
        "                                args, skip_pixel_loss, model, imgs, imgs_pred,\n",
        "                                boxes, boxes_pred, masks, masks_pred,\n",
        "                                predicates, predicate_scores)\n",
        "\n",
        "      total_iou += jaccard(boxes_pred, boxes)\n",
        "      total_boxes += boxes_pred.size(0)\n",
        "\n",
        "      for loss_name, loss_val in losses.items():\n",
        "        all_losses[loss_name].append(loss_val)\n",
        "      num_samples += imgs.size(0)\n",
        "      if num_samples >= args.num_val_samples:\n",
        "        break\n",
        "\n",
        "    samples = {}\n",
        "    samples['gt_img'] = imgs\n",
        "\n",
        "    model_out = model(objs, triples, obj_to_img, boxes_gt=boxes, masks_gt=masks)\n",
        "    samples['gt_box_gt_mask'] = model_out[0]\n",
        "\n",
        "    model_out = model(objs, triples, obj_to_img, boxes_gt=boxes)\n",
        "    samples['gt_box_pred_mask'] = model_out[0]\n",
        "\n",
        "    model_out = model(objs, triples, obj_to_img)\n",
        "    samples['pred_box_pred_mask'] = model_out[0]\n",
        "\n",
        "    for k, v in samples.items():\n",
        "      samples[k] = imagenet_deprocess_batch(v)\n",
        "\n",
        "    mean_losses = {k: np.mean(v) for k, v in all_losses.items()}\n",
        "    avg_iou = total_iou / total_boxes\n",
        "\n",
        "    masks_to_store = masks\n",
        "    if masks_to_store is not None:\n",
        "      masks_to_store = masks_to_store.data.cpu().clone()\n",
        "\n",
        "    masks_pred_to_store = masks_pred\n",
        "    if masks_pred_to_store is not None:\n",
        "      masks_pred_to_store = masks_pred_to_store.data.cpu().clone()\n",
        "\n",
        "  batch_data = {\n",
        "    'objs': objs.detach().cpu().clone(),\n",
        "    'boxes_gt': boxes.detach().cpu().clone(), \n",
        "    'masks_gt': masks_to_store,\n",
        "    'triples': triples.detach().cpu().clone(),\n",
        "    'obj_to_img': obj_to_img.detach().cpu().clone(),\n",
        "    'triple_to_img': triple_to_img.detach().cpu().clone(),\n",
        "    'boxes_pred': boxes_pred.detach().cpu().clone(),\n",
        "    'masks_pred': masks_pred_to_store\n",
        "  }\n",
        "  out = [mean_losses, samples, batch_data, avg_iou]\n",
        "\n",
        "  return tuple(out)\n",
        "\n",
        "\n",
        "def calculate_model_losses(args, skip_pixel_loss, model, img, img_pred,\n",
        "                           bbox, bbox_pred, masks, masks_pred,\n",
        "                           predicates, predicate_scores):\n",
        "  total_loss = torch.zeros(1).to(img)\n",
        "  losses = {}\n",
        "\n",
        "  l1_pixel_weight = args.l1_pixel_loss_weight\n",
        "  if skip_pixel_loss:\n",
        "    l1_pixel_weight = 0\n",
        "  l1_pixel_loss = F.l1_loss(img_pred, img)\n",
        "  total_loss = add_loss(total_loss, l1_pixel_loss, losses, 'L1_pixel_loss',\n",
        "                        l1_pixel_weight)\n",
        "  loss_bbox = F.mse_loss(bbox_pred, bbox)\n",
        "  total_loss = add_loss(total_loss, loss_bbox, losses, 'bbox_pred',\n",
        "                        args.bbox_pred_loss_weight)\n",
        "\n",
        "  if args.predicate_pred_loss_weight > 0:\n",
        "    loss_predicate = F.cross_entropy(predicate_scores, predicates)\n",
        "    total_loss = add_loss(total_loss, loss_predicate, losses, 'predicate_pred',\n",
        "                          args.predicate_pred_loss_weight)\n",
        "\n",
        "  if args.mask_loss_weight > 0 and masks is not None and masks_pred is not None:\n",
        "    mask_loss = F.binary_cross_entropy(masks_pred, masks.float())\n",
        "    total_loss = add_loss(total_loss, mask_loss, losses, 'mask_loss',\n",
        "                          args.mask_loss_weight)\n",
        "  return total_loss, losses\n",
        "\n",
        "\n",
        "def main(args):\n",
        "  print(args)\n",
        "  check_args(args)\n",
        "  float_dtype = torch.cuda.FloatTensor\n",
        "  long_dtype = torch.cuda.LongTensor\n",
        "\n",
        "  vocab, train_loader, val_loader = build_loaders(args)\n",
        "  model, model_kwargs = build_model(args, vocab)\n",
        "  model.type(float_dtype)\n",
        "  print(model)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "  obj_discriminator, d_obj_kwargs = build_obj_discriminator(args, vocab)\n",
        "  img_discriminator, d_img_kwargs = build_img_discriminator(args, vocab)\n",
        "  gan_g_loss, gan_d_loss = get_gan_losses(args.gan_loss_type)\n",
        "\n",
        "  if obj_discriminator is not None:\n",
        "    obj_discriminator.type(float_dtype)\n",
        "    obj_discriminator.train()\n",
        "    print(obj_discriminator)\n",
        "    optimizer_d_obj = torch.optim.Adam(obj_discriminator.parameters(),\n",
        "                                       lr=args.learning_rate)\n",
        "\n",
        "  if img_discriminator is not None:\n",
        "    img_discriminator.type(float_dtype)\n",
        "    img_discriminator.train()\n",
        "    print(img_discriminator)\n",
        "    optimizer_d_img = torch.optim.Adam(img_discriminator.parameters(),\n",
        "                                       lr=args.learning_rate)\n",
        "\n",
        "  restore_path = None\n",
        "  if args.restore_from_checkpoint:\n",
        "    restore_path = '%s_with_model.pt' % args.checkpoint_name\n",
        "    restore_path = os.path.join(args.output_dir, restore_path)\n",
        "  if restore_path is not None and os.path.isfile(restore_path):\n",
        "    print('Restoring from checkpoint:')\n",
        "    print(restore_path)\n",
        "    checkpoint = torch.load(restore_path)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    optimizer.load_state_dict(checkpoint['optim_state'])\n",
        "\n",
        "    if obj_discriminator is not None:\n",
        "      obj_discriminator.load_state_dict(checkpoint['d_obj_state'])\n",
        "      optimizer_d_obj.load_state_dict(checkpoint['d_obj_optim_state'])\n",
        "\n",
        "    if img_discriminator is not None:\n",
        "      img_discriminator.load_state_dict(checkpoint['d_img_state'])\n",
        "      optimizer_d_img.load_state_dict(checkpoint['d_img_optim_state'])\n",
        "\n",
        "    t = checkpoint['counters']['t']\n",
        "    if 0 <= args.eval_mode_after <= t:\n",
        "      model.eval()\n",
        "    else:\n",
        "      model.train()\n",
        "    epoch = checkpoint['counters']['epoch']\n",
        "  else:\n",
        "    t, epoch = 0, 0\n",
        "    checkpoint = {\n",
        "      'args': args.__dict__,\n",
        "      'vocab': vocab,\n",
        "      'model_kwargs': model_kwargs,\n",
        "      'd_obj_kwargs': d_obj_kwargs,\n",
        "      'd_img_kwargs': d_img_kwargs,\n",
        "      'losses_ts': [],\n",
        "      'losses': defaultdict(list),\n",
        "      'd_losses': defaultdict(list),\n",
        "      'checkpoint_ts': [],\n",
        "      'train_batch_data': [], \n",
        "      'train_samples': [],\n",
        "      'train_iou': [],\n",
        "      'val_batch_data': [], \n",
        "      'val_samples': [],\n",
        "      'val_losses': defaultdict(list),\n",
        "      'val_iou': [], \n",
        "      'norm_d': [], \n",
        "      'norm_g': [],\n",
        "      'counters': {\n",
        "        't': None,\n",
        "        'epoch': None,\n",
        "      },\n",
        "      'model_state': None, 'model_best_state': None, 'optim_state': None,\n",
        "      'd_obj_state': None, 'd_obj_best_state': None, 'd_obj_optim_state': None,\n",
        "      'd_img_state': None, 'd_img_best_state': None, 'd_img_optim_state': None,\n",
        "      'best_t': [],\n",
        "    }\n",
        "\n",
        "  while True:\n",
        "    if t >= args.num_iterations:\n",
        "      break\n",
        "    epoch += 1\n",
        "    print('Starting epoch %d' % epoch)\n",
        "    \n",
        "    for batch in train_loader:\n",
        "      if t == args.eval_mode_after:\n",
        "        print('switching to eval mode')\n",
        "        model.eval()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "      t += 1\n",
        "      batch = [tensor.cuda() for tensor in batch]\n",
        "      masks = None\n",
        "      if len(batch) == 6:\n",
        "        imgs, objs, boxes, triples, obj_to_img, triple_to_img = batch\n",
        "      elif len(batch) == 7:\n",
        "        imgs, objs, boxes, masks, triples, obj_to_img, triple_to_img = batch\n",
        "      else:\n",
        "        assert False\n",
        "      predicates = triples[:, 1]\n",
        "\n",
        "      with timeit('forward', args.timing):\n",
        "        model_boxes = boxes\n",
        "        model_masks = masks\n",
        "        model_out = model(objs, triples, obj_to_img,\n",
        "                          boxes_gt=model_boxes, masks_gt=model_masks)\n",
        "        imgs_pred, boxes_pred, masks_pred, predicate_scores = model_out\n",
        "      with timeit('loss', args.timing):\n",
        "        # Skip the pixel loss if using GT boxes\n",
        "        skip_pixel_loss = (model_boxes is None)\n",
        "        total_loss, losses =  calculate_model_losses(\n",
        "                                args, skip_pixel_loss, model, imgs, imgs_pred,\n",
        "                                boxes, boxes_pred, masks, masks_pred,\n",
        "                                predicates, predicate_scores)\n",
        "\n",
        "      if obj_discriminator is not None:\n",
        "        scores_fake, ac_loss = obj_discriminator(imgs_pred, objs, boxes, obj_to_img)\n",
        "        total_loss = add_loss(total_loss, ac_loss, losses, 'ac_loss',\n",
        "                              args.ac_loss_weight)\n",
        "        weight = args.discriminator_loss_weight * args.d_obj_weight\n",
        "        total_loss = add_loss(total_loss, gan_g_loss(scores_fake), losses,\n",
        "                              'g_gan_obj_loss', weight)\n",
        "\n",
        "      if img_discriminator is not None:\n",
        "        scores_fake = img_discriminator(imgs_pred)\n",
        "        weight = args.discriminator_loss_weight * args.d_img_weight\n",
        "        total_loss = add_loss(total_loss, gan_g_loss(scores_fake), losses,\n",
        "                              'g_gan_img_loss', weight)\n",
        "\n",
        "      losses['total_loss'] = total_loss.item()\n",
        "      if not math.isfinite(losses['total_loss']):\n",
        "        print('WARNING: Got loss = NaN, not backpropping')\n",
        "        continue\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      with timeit('backward', args.timing):\n",
        "        total_loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss_d = None\n",
        "      ac_loss_real = None\n",
        "      ac_loss_fake = None\n",
        "      d_losses = {}\n",
        "      \n",
        "      if obj_discriminator is not None:\n",
        "        d_obj_losses = LossManager()\n",
        "        imgs_fake = imgs_pred.detach()\n",
        "        scores_fake, ac_loss_fake = obj_discriminator(imgs_fake, objs, boxes, obj_to_img)\n",
        "        scores_real, ac_loss_real = obj_discriminator(imgs, objs, boxes, obj_to_img)\n",
        "\n",
        "        d_obj_gan_loss = gan_d_loss(scores_real, scores_fake)\n",
        "        d_obj_losses.add_loss(d_obj_gan_loss, 'd_obj_gan_loss')\n",
        "        d_obj_losses.add_loss(ac_loss_real, 'd_ac_loss_real')\n",
        "        d_obj_losses.add_loss(ac_loss_fake, 'd_ac_loss_fake')\n",
        "\n",
        "        optimizer_d_obj.zero_grad()\n",
        "        d_obj_losses.total_loss.backward()\n",
        "        optimizer_d_obj.step()\n",
        "\n",
        "      if img_discriminator is not None:\n",
        "        d_img_losses = LossManager()\n",
        "        imgs_fake = imgs_pred.detach()\n",
        "        scores_fake = img_discriminator(imgs_fake)\n",
        "        scores_real = img_discriminator(imgs)\n",
        "\n",
        "        d_img_gan_loss = gan_d_loss(scores_real, scores_fake)\n",
        "        d_img_losses.add_loss(d_img_gan_loss, 'd_img_gan_loss')\n",
        "        \n",
        "        optimizer_d_img.zero_grad()\n",
        "        d_img_losses.total_loss.backward()\n",
        "        optimizer_d_img.step()\n",
        "\n",
        "      if t % args.print_every == 0:\n",
        "        print('t = %d / %d' % (t, args.num_iterations))\n",
        "        for name, val in losses.items():\n",
        "          print(' G [%s]: %.4f' % (name, val))\n",
        "          checkpoint['losses'][name].append(val)\n",
        "        checkpoint['losses_ts'].append(t)\n",
        "\n",
        "        if obj_discriminator is not None:\n",
        "          for name, val in d_obj_losses.items():\n",
        "            print(' D_obj [%s]: %.4f' % (name, val))\n",
        "            checkpoint['d_losses'][name].append(val)\n",
        "\n",
        "        if img_discriminator is not None:\n",
        "          for name, val in d_img_losses.items():\n",
        "            print(' D_img [%s]: %.4f' % (name, val))\n",
        "            checkpoint['d_losses'][name].append(val)\n",
        "      \n",
        "      if t % args.checkpoint_every == 0:\n",
        "        print('checking on train')\n",
        "        train_results = check_model(args, t, train_loader, model)\n",
        "        t_losses, t_samples, t_batch_data, t_avg_iou = train_results\n",
        "\n",
        "        checkpoint['train_batch_data'].append(t_batch_data)\n",
        "        checkpoint['train_samples'].append(t_samples)\n",
        "        checkpoint['checkpoint_ts'].append(t)\n",
        "        checkpoint['train_iou'].append(t_avg_iou)\n",
        "\n",
        "        print('checking on val')\n",
        "        val_results = check_model(args, t, val_loader, model)\n",
        "        val_losses, val_samples, val_batch_data, val_avg_iou = val_results\n",
        "        checkpoint['val_samples'].append(val_samples)\n",
        "        checkpoint['val_batch_data'].append(val_batch_data)\n",
        "        checkpoint['val_iou'].append(val_avg_iou)\n",
        "\n",
        "        print('train iou: ', t_avg_iou)\n",
        "        print('val iou: ', val_avg_iou)\n",
        "\n",
        "        for k, v in val_losses.items():\n",
        "          checkpoint['val_losses'][k].append(v)\n",
        "        checkpoint['model_state'] = model.state_dict()\n",
        "\n",
        "        if obj_discriminator is not None:\n",
        "          checkpoint['d_obj_state'] = obj_discriminator.state_dict()\n",
        "          checkpoint['d_obj_optim_state'] = optimizer_d_obj.state_dict()\n",
        "\n",
        "        if img_discriminator is not None:\n",
        "          checkpoint['d_img_state'] = img_discriminator.state_dict()\n",
        "          checkpoint['d_img_optim_state'] = optimizer_d_img.state_dict()\n",
        "\n",
        "        checkpoint['optim_state'] = optimizer.state_dict()\n",
        "        checkpoint['counters']['t'] = t\n",
        "        checkpoint['counters']['epoch'] = epoch\n",
        "        checkpoint_path = os.path.join(args.output_dir,\n",
        "                              '%s_with_model.pt' % args.checkpoint_name)\n",
        "        print('Saving checkpoint to ', checkpoint_path)\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        # Save another checkpoint without any model or optim state\n",
        "        checkpoint_path = os.path.join(args.output_dir,\n",
        "                              '%s_no_model.pt' % args.checkpoint_name)\n",
        "        key_blacklist = ['model_state', 'optim_state', 'model_best_state',\n",
        "                         'd_obj_state', 'd_obj_optim_state', 'd_obj_best_state',\n",
        "                         'd_img_state', 'd_img_optim_state', 'd_img_best_state']\n",
        "        small_checkpoint = {}\n",
        "        for k, v in checkpoint.items():\n",
        "          if k not in key_blacklist:\n",
        "            small_checkpoint[k] = v\n",
        "        torch.save(small_checkpoint, checkpoint_path)\n",
        "\n",
        "import sys\n",
        "sys.argv=[sys.argv[0], '--dataset','vg']\n",
        "\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "  args = parser.parse_args()\n",
        "  main(args)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(ac_loss_weight=0.1, activation='leakyrelu-0.2', batch_size=32, bbox_pred_loss_weight=10, checkpoint_every=10000, checkpoint_name='checkpoint', checkpoint_start_from=None, coco_include_other=False, coco_stuff_only=True, coco_train_image_dir='datasets/coco/images/train2017', coco_train_instances_json='datasets/coco/annotations/instances_train2017.json', coco_train_stuff_json='datasets/coco/annotations/stuff_train2017.json', coco_val_image_dir='datasets/coco/images/val2017', coco_val_instances_json='datasets/coco/annotations/instances_val2017.json', coco_val_stuff_json='datasets/coco/annotations/stuff_val2017.json', crop_size=32, d_activation='leakyrelu-0.2', d_clip=None, d_img_arch='C4-64-2,C4-128-2,C4-256-2', d_img_weight=1.0, d_normalization='batch', d_obj_arch='C4-64-2,C4-128-2,C4-256-2', d_obj_weight=1.0, d_padding='valid', dataset='vg', discriminator_loss_weight=0.01, embedding_dim=128, eval_mode_after=100000, gan_loss_type='gan', gconv_dim=128, gconv_hidden_dim=512, gconv_num_layers=5, image_size=(64, 64), include_relationships=True, instance_whitelist=None, l1_pixel_loss_weight=1.0, layout_noise_dim=32, learning_rate=0.0001, loader_num_workers=4, mask_loss_weight=0, mask_size=16, max_objects_per_image=10, min_object_size=0.02, min_objects_per_image=3, mlp_normalization='none', normalization='batch', num_iterations=1000000, num_train_samples=None, num_val_samples=1024, output_dir='/content/sg2im', predicate_pred_loss_weight=0, print_every=10, refinement_network_dims=(1024, 512, 256, 128, 64), restore_from_checkpoint=False, shuffle_val=True, stuff_whitelist=None, timing=False, train_h5='datasets/vg/train.h5', use_boxes_pred_after=-1, val_h5='datasets/vg/val.h5', vg_image_dir='datasets/vg/images', vg_use_orphaned_objects=True, vocab_json='datasets/vg/vocab.json')\n",
            "There are 1955 iterations per epoch\n",
            "Sg2ImModel(\n",
            "  (obj_embeddings): Embedding(180, 128)\n",
            "  (pred_embeddings): Embedding(46, 128)\n",
            "  (gconv): GraphTripleConv(\n",
            "    (net1): Sequential(\n",
            "      (0): Linear(in_features=384, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=512, out_features=1152, bias=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (net2): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (gconv_net): GraphTripleConvNet(\n",
            "    (gconvs): ModuleList(\n",
            "      (0): GraphTripleConv(\n",
            "        (net1): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "        (net2): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (1): GraphTripleConv(\n",
            "        (net1): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "        (net2): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (2): GraphTripleConv(\n",
            "        (net1): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "        (net2): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "      (3): GraphTripleConv(\n",
            "        (net1): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "        (net2): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "          (3): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (box_net): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (mask_net): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU()\n",
            "    (12): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU()\n",
            "    (16): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (rel_aux_net): Sequential(\n",
            "    (0): Linear(in_features=264, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=46, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (refinement_net): RefinementNetwork(\n",
            "    (refinement_modules): ModuleList(\n",
            "      (0): RefinementModule(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(161, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "      (1): RefinementModule(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(1184, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "      (2): RefinementModule(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(672, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "      (3): RefinementModule(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(416, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "      (4): RefinementModule(\n",
            "        (net): Sequential(\n",
            "          (0): Conv2d(288, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (output_conv): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "LeakyReLU(negative_slope=0.2)\n",
            "Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "LeakyReLU(negative_slope=0.2)\n",
            "Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
            "Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "LeakyReLU(negative_slope=0.2)\n",
            "Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
            "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "LeakyReLU(negative_slope=0.2)\n",
            "Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
            "AcCropDiscriminator(\n",
            "  (discriminator): AcDiscriminator(\n",
            "    (cnn): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.2)\n",
            "        (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.2)\n",
            "        (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
            "      )\n",
            "      (1): GlobalAvgPool()\n",
            "      (2): Linear(in_features=256, out_features=1024, bias=True)\n",
            "    )\n",
            "    (real_classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
            "    (obj_classifier): Linear(in_features=1024, out_features=179, bias=True)\n",
            "  )\n",
            ")\n",
            "PatchDiscriminator(\n",
            "  (cnn): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
            "  )\n",
            "  (classifier): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n",
            "Starting epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2390: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "t = 10 / 1000000\n",
            " G [L1_pixel_loss]: 0.9335\n",
            " G [bbox_pred]: 1.7017\n",
            " G [ac_loss]: 0.4769\n",
            " G [g_gan_obj_loss]: 0.0071\n",
            " G [g_gan_img_loss]: 0.0071\n",
            " G [total_loss]: 3.1263\n",
            " D_obj [d_obj_gan_loss]: 1.3654\n",
            " D_obj [d_ac_loss_real]: 4.7558\n",
            " D_obj [d_ac_loss_fake]: 4.7687\n",
            " D_img [d_img_gan_loss]: 1.4071\n",
            "t = 20 / 1000000\n",
            " G [L1_pixel_loss]: 1.0017\n",
            " G [bbox_pred]: 0.6375\n",
            " G [ac_loss]: 0.4543\n",
            " G [g_gan_obj_loss]: 0.0072\n",
            " G [g_gan_img_loss]: 0.0072\n",
            " G [total_loss]: 2.1080\n",
            " D_obj [d_obj_gan_loss]: 1.3428\n",
            " D_obj [d_ac_loss_real]: 4.5598\n",
            " D_obj [d_ac_loss_fake]: 4.5429\n",
            " D_img [d_img_gan_loss]: 1.3824\n",
            "t = 30 / 1000000\n",
            " G [L1_pixel_loss]: 0.9930\n",
            " G [bbox_pred]: 0.6839\n",
            " G [ac_loss]: 0.4435\n",
            " G [g_gan_obj_loss]: 0.0077\n",
            " G [g_gan_img_loss]: 0.0076\n",
            " G [total_loss]: 2.1356\n",
            " D_obj [d_obj_gan_loss]: 1.2825\n",
            " D_obj [d_ac_loss_real]: 4.5150\n",
            " D_obj [d_ac_loss_fake]: 4.4354\n",
            " D_img [d_img_gan_loss]: 1.3398\n",
            "t = 40 / 1000000\n",
            " G [L1_pixel_loss]: 0.9777\n",
            " G [bbox_pred]: 0.5730\n",
            " G [ac_loss]: 0.4058\n",
            " G [g_gan_obj_loss]: 0.0084\n",
            " G [g_gan_img_loss]: 0.0080\n",
            " G [total_loss]: 1.9729\n",
            " D_obj [d_obj_gan_loss]: 1.2371\n",
            " D_obj [d_ac_loss_real]: 4.0776\n",
            " D_obj [d_ac_loss_fake]: 4.0585\n",
            " D_img [d_img_gan_loss]: 1.3087\n",
            "t = 50 / 1000000\n",
            " G [L1_pixel_loss]: 0.9244\n",
            " G [bbox_pred]: 0.6139\n",
            " G [ac_loss]: 0.4207\n",
            " G [g_gan_obj_loss]: 0.0090\n",
            " G [g_gan_img_loss]: 0.0081\n",
            " G [total_loss]: 1.9760\n",
            " D_obj [d_obj_gan_loss]: 1.1392\n",
            " D_obj [d_ac_loss_real]: 4.4123\n",
            " D_obj [d_ac_loss_fake]: 4.2072\n",
            " D_img [d_img_gan_loss]: 1.2439\n",
            "t = 60 / 1000000\n",
            " G [L1_pixel_loss]: 0.9396\n",
            " G [bbox_pred]: 0.6293\n",
            " G [ac_loss]: 0.4087\n",
            " G [g_gan_obj_loss]: 0.0092\n",
            " G [g_gan_img_loss]: 0.0081\n",
            " G [total_loss]: 1.9949\n",
            " D_obj [d_obj_gan_loss]: 1.1147\n",
            " D_obj [d_ac_loss_real]: 4.3308\n",
            " D_obj [d_ac_loss_fake]: 4.0868\n",
            " D_img [d_img_gan_loss]: 1.2352\n",
            "t = 70 / 1000000\n",
            " G [L1_pixel_loss]: 0.9164\n",
            " G [bbox_pred]: 0.6058\n",
            " G [ac_loss]: 0.4219\n",
            " G [g_gan_obj_loss]: 0.0089\n",
            " G [g_gan_img_loss]: 0.0083\n",
            " G [total_loss]: 1.9613\n",
            " D_obj [d_obj_gan_loss]: 1.0915\n",
            " D_obj [d_ac_loss_real]: 4.4764\n",
            " D_obj [d_ac_loss_fake]: 4.2194\n",
            " D_img [d_img_gan_loss]: 1.2297\n",
            "t = 80 / 1000000\n",
            " G [L1_pixel_loss]: 0.9028\n",
            " G [bbox_pred]: 0.5590\n",
            " G [ac_loss]: 0.3879\n",
            " G [g_gan_obj_loss]: 0.0094\n",
            " G [g_gan_img_loss]: 0.0081\n",
            " G [total_loss]: 1.8671\n",
            " D_obj [d_obj_gan_loss]: 1.0220\n",
            " D_obj [d_ac_loss_real]: 4.2432\n",
            " D_obj [d_ac_loss_fake]: 3.8793\n",
            " D_img [d_img_gan_loss]: 1.2377\n",
            "t = 90 / 1000000\n",
            " G [L1_pixel_loss]: 0.8879\n",
            " G [bbox_pred]: 0.5098\n",
            " G [ac_loss]: 0.3811\n",
            " G [g_gan_obj_loss]: 0.0109\n",
            " G [g_gan_img_loss]: 0.0081\n",
            " G [total_loss]: 1.7978\n",
            " D_obj [d_obj_gan_loss]: 0.9961\n",
            " D_obj [d_ac_loss_real]: 4.1811\n",
            " D_obj [d_ac_loss_fake]: 3.8112\n",
            " D_img [d_img_gan_loss]: 1.2841\n",
            "t = 100 / 1000000\n",
            " G [L1_pixel_loss]: 0.9266\n",
            " G [bbox_pred]: 0.5160\n",
            " G [ac_loss]: 0.3897\n",
            " G [g_gan_obj_loss]: 0.0093\n",
            " G [g_gan_img_loss]: 0.0075\n",
            " G [total_loss]: 1.8491\n",
            " D_obj [d_obj_gan_loss]: 1.1413\n",
            " D_obj [d_ac_loss_real]: 4.2027\n",
            " D_obj [d_ac_loss_fake]: 3.8969\n",
            " D_img [d_img_gan_loss]: 1.3970\n",
            "t = 110 / 1000000\n",
            " G [L1_pixel_loss]: 0.9085\n",
            " G [bbox_pred]: 0.4914\n",
            " G [ac_loss]: 0.3704\n",
            " G [g_gan_obj_loss]: 0.0111\n",
            " G [g_gan_img_loss]: 0.0085\n",
            " G [total_loss]: 1.7900\n",
            " D_obj [d_obj_gan_loss]: 1.0317\n",
            " D_obj [d_ac_loss_real]: 4.1293\n",
            " D_obj [d_ac_loss_fake]: 3.7044\n",
            " D_img [d_img_gan_loss]: 1.2534\n",
            "t = 120 / 1000000\n",
            " G [L1_pixel_loss]: 0.9180\n",
            " G [bbox_pred]: 0.5006\n",
            " G [ac_loss]: 0.3795\n",
            " G [g_gan_obj_loss]: 0.0118\n",
            " G [g_gan_img_loss]: 0.0080\n",
            " G [total_loss]: 1.8179\n",
            " D_obj [d_obj_gan_loss]: 1.0502\n",
            " D_obj [d_ac_loss_real]: 4.1527\n",
            " D_obj [d_ac_loss_fake]: 3.7953\n",
            " D_img [d_img_gan_loss]: 1.2045\n",
            "t = 130 / 1000000\n",
            " G [L1_pixel_loss]: 0.9227\n",
            " G [bbox_pred]: 0.4824\n",
            " G [ac_loss]: 0.3594\n",
            " G [g_gan_obj_loss]: 0.0123\n",
            " G [g_gan_img_loss]: 0.0075\n",
            " G [total_loss]: 1.7843\n",
            " D_obj [d_obj_gan_loss]: 1.3490\n",
            " D_obj [d_ac_loss_real]: 3.8636\n",
            " D_obj [d_ac_loss_fake]: 3.5939\n",
            " D_img [d_img_gan_loss]: 1.3188\n",
            "t = 140 / 1000000\n",
            " G [L1_pixel_loss]: 0.9559\n",
            " G [bbox_pred]: 0.5547\n",
            " G [ac_loss]: 0.3941\n",
            " G [g_gan_obj_loss]: 0.0136\n",
            " G [g_gan_img_loss]: 0.0089\n",
            " G [total_loss]: 1.9271\n",
            " D_obj [d_obj_gan_loss]: 0.7941\n",
            " D_obj [d_ac_loss_real]: 4.3659\n",
            " D_obj [d_ac_loss_fake]: 3.9407\n",
            " D_img [d_img_gan_loss]: 1.2017\n",
            "t = 150 / 1000000\n",
            " G [L1_pixel_loss]: 0.9237\n",
            " G [bbox_pred]: 0.4882\n",
            " G [ac_loss]: 0.3681\n",
            " G [g_gan_obj_loss]: 0.0133\n",
            " G [g_gan_img_loss]: 0.0089\n",
            " G [total_loss]: 1.8022\n",
            " D_obj [d_obj_gan_loss]: 0.8027\n",
            " D_obj [d_ac_loss_real]: 4.2600\n",
            " D_obj [d_ac_loss_fake]: 3.6810\n",
            " D_img [d_img_gan_loss]: 1.1300\n",
            "t = 160 / 1000000\n",
            " G [L1_pixel_loss]: 0.9524\n",
            " G [bbox_pred]: 0.5196\n",
            " G [ac_loss]: 0.3450\n",
            " G [g_gan_obj_loss]: 0.0167\n",
            " G [g_gan_img_loss]: 0.0081\n",
            " G [total_loss]: 1.8418\n",
            " D_obj [d_obj_gan_loss]: 0.8732\n",
            " D_obj [d_ac_loss_real]: 4.0001\n",
            " D_obj [d_ac_loss_fake]: 3.4496\n",
            " D_img [d_img_gan_loss]: 1.2019\n",
            "t = 170 / 1000000\n",
            " G [L1_pixel_loss]: 0.9090\n",
            " G [bbox_pred]: 0.5309\n",
            " G [ac_loss]: 0.3437\n",
            " G [g_gan_obj_loss]: 0.0137\n",
            " G [g_gan_img_loss]: 0.0086\n",
            " G [total_loss]: 1.8059\n",
            " D_obj [d_obj_gan_loss]: 0.7149\n",
            " D_obj [d_ac_loss_real]: 4.1440\n",
            " D_obj [d_ac_loss_fake]: 3.4367\n",
            " D_img [d_img_gan_loss]: 1.2164\n",
            "t = 180 / 1000000\n",
            " G [L1_pixel_loss]: 0.9159\n",
            " G [bbox_pred]: 0.4647\n",
            " G [ac_loss]: 0.3584\n",
            " G [g_gan_obj_loss]: 0.0143\n",
            " G [g_gan_img_loss]: 0.0087\n",
            " G [total_loss]: 1.7621\n",
            " D_obj [d_obj_gan_loss]: 0.8005\n",
            " D_obj [d_ac_loss_real]: 4.2457\n",
            " D_obj [d_ac_loss_fake]: 3.5844\n",
            " D_img [d_img_gan_loss]: 1.2849\n",
            "t = 190 / 1000000\n",
            " G [L1_pixel_loss]: 0.8519\n",
            " G [bbox_pred]: 0.5259\n",
            " G [ac_loss]: 0.3673\n",
            " G [g_gan_obj_loss]: 0.0186\n",
            " G [g_gan_img_loss]: 0.0087\n",
            " G [total_loss]: 1.7723\n",
            " D_obj [d_obj_gan_loss]: 0.8274\n",
            " D_obj [d_ac_loss_real]: 4.2080\n",
            " D_obj [d_ac_loss_fake]: 3.6731\n",
            " D_img [d_img_gan_loss]: 1.2473\n",
            "t = 200 / 1000000\n",
            " G [L1_pixel_loss]: 0.9168\n",
            " G [bbox_pred]: 0.4757\n",
            " G [ac_loss]: 0.3424\n",
            " G [g_gan_obj_loss]: 0.0155\n",
            " G [g_gan_img_loss]: 0.0092\n",
            " G [total_loss]: 1.7595\n",
            " D_obj [d_obj_gan_loss]: 0.7407\n",
            " D_obj [d_ac_loss_real]: 4.0942\n",
            " D_obj [d_ac_loss_fake]: 3.4237\n",
            " D_img [d_img_gan_loss]: 1.1076\n",
            "t = 210 / 1000000\n",
            " G [L1_pixel_loss]: 0.9528\n",
            " G [bbox_pred]: 0.5534\n",
            " G [ac_loss]: 0.3278\n",
            " G [g_gan_obj_loss]: 0.0172\n",
            " G [g_gan_img_loss]: 0.0074\n",
            " G [total_loss]: 1.8587\n",
            " D_obj [d_obj_gan_loss]: 0.7179\n",
            " D_obj [d_ac_loss_real]: 3.9708\n",
            " D_obj [d_ac_loss_fake]: 3.2777\n",
            " D_img [d_img_gan_loss]: 1.2752\n",
            "t = 220 / 1000000\n",
            " G [L1_pixel_loss]: 0.9004\n",
            " G [bbox_pred]: 0.5186\n",
            " G [ac_loss]: 0.3441\n",
            " G [g_gan_obj_loss]: 0.0171\n",
            " G [g_gan_img_loss]: 0.0087\n",
            " G [total_loss]: 1.7889\n",
            " D_obj [d_obj_gan_loss]: 0.8229\n",
            " D_obj [d_ac_loss_real]: 4.1572\n",
            " D_obj [d_ac_loss_fake]: 3.4409\n",
            " D_img [d_img_gan_loss]: 1.2643\n",
            "t = 230 / 1000000\n",
            " G [L1_pixel_loss]: 0.9193\n",
            " G [bbox_pred]: 0.4831\n",
            " G [ac_loss]: 0.3357\n",
            " G [g_gan_obj_loss]: 0.0150\n",
            " G [g_gan_img_loss]: 0.0091\n",
            " G [total_loss]: 1.7621\n",
            " D_obj [d_obj_gan_loss]: 0.9803\n",
            " D_obj [d_ac_loss_real]: 4.2085\n",
            " D_obj [d_ac_loss_fake]: 3.3571\n",
            " D_img [d_img_gan_loss]: 1.1880\n",
            "t = 240 / 1000000\n",
            " G [L1_pixel_loss]: 0.8714\n",
            " G [bbox_pred]: 0.5332\n",
            " G [ac_loss]: 0.3391\n",
            " G [g_gan_obj_loss]: 0.0157\n",
            " G [g_gan_img_loss]: 0.0077\n",
            " G [total_loss]: 1.7670\n",
            " D_obj [d_obj_gan_loss]: 0.6177\n",
            " D_obj [d_ac_loss_real]: 4.1628\n",
            " D_obj [d_ac_loss_fake]: 3.3907\n",
            " D_img [d_img_gan_loss]: 1.2427\n",
            "t = 250 / 1000000\n",
            " G [L1_pixel_loss]: 0.9280\n",
            " G [bbox_pred]: 0.4934\n",
            " G [ac_loss]: 0.3192\n",
            " G [g_gan_obj_loss]: 0.0200\n",
            " G [g_gan_img_loss]: 0.0089\n",
            " G [total_loss]: 1.7696\n",
            " D_obj [d_obj_gan_loss]: 0.5674\n",
            " D_obj [d_ac_loss_real]: 3.9609\n",
            " D_obj [d_ac_loss_fake]: 3.1917\n",
            " D_img [d_img_gan_loss]: 1.2277\n",
            "t = 260 / 1000000\n",
            " G [L1_pixel_loss]: 0.8769\n",
            " G [bbox_pred]: 0.5112\n",
            " G [ac_loss]: 0.3139\n",
            " G [g_gan_obj_loss]: 0.0171\n",
            " G [g_gan_img_loss]: 0.0077\n",
            " G [total_loss]: 1.7268\n",
            " D_obj [d_obj_gan_loss]: 0.6818\n",
            " D_obj [d_ac_loss_real]: 4.0355\n",
            " D_obj [d_ac_loss_fake]: 3.1391\n",
            " D_img [d_img_gan_loss]: 1.2147\n",
            "t = 270 / 1000000\n",
            " G [L1_pixel_loss]: 0.9236\n",
            " G [bbox_pred]: 0.4660\n",
            " G [ac_loss]: 0.3119\n",
            " G [g_gan_obj_loss]: 0.0168\n",
            " G [g_gan_img_loss]: 0.0094\n",
            " G [total_loss]: 1.7277\n",
            " D_obj [d_obj_gan_loss]: 0.7372\n",
            " D_obj [d_ac_loss_real]: 3.8870\n",
            " D_obj [d_ac_loss_fake]: 3.1190\n",
            " D_img [d_img_gan_loss]: 1.0579\n",
            "t = 280 / 1000000\n",
            " G [L1_pixel_loss]: 0.8754\n",
            " G [bbox_pred]: 0.5210\n",
            " G [ac_loss]: 0.2973\n",
            " G [g_gan_obj_loss]: 0.0194\n",
            " G [g_gan_img_loss]: 0.0088\n",
            " G [total_loss]: 1.7220\n",
            " D_obj [d_obj_gan_loss]: 0.7510\n",
            " D_obj [d_ac_loss_real]: 4.2289\n",
            " D_obj [d_ac_loss_fake]: 2.9727\n",
            " D_img [d_img_gan_loss]: 1.1385\n",
            "t = 290 / 1000000\n",
            " G [L1_pixel_loss]: 0.9059\n",
            " G [bbox_pred]: 0.5406\n",
            " G [ac_loss]: 0.3133\n",
            " G [g_gan_obj_loss]: 0.0149\n",
            " G [g_gan_img_loss]: 0.0088\n",
            " G [total_loss]: 1.7836\n",
            " D_obj [d_obj_gan_loss]: 0.7018\n",
            " D_obj [d_ac_loss_real]: 4.0474\n",
            " D_obj [d_ac_loss_fake]: 3.1333\n",
            " D_img [d_img_gan_loss]: 1.1940\n",
            "t = 300 / 1000000\n",
            " G [L1_pixel_loss]: 0.9605\n",
            " G [bbox_pred]: 0.5134\n",
            " G [ac_loss]: 0.3140\n",
            " G [g_gan_obj_loss]: 0.0151\n",
            " G [g_gan_img_loss]: 0.0094\n",
            " G [total_loss]: 1.8124\n",
            " D_obj [d_obj_gan_loss]: 0.5599\n",
            " D_obj [d_ac_loss_real]: 4.1294\n",
            " D_obj [d_ac_loss_fake]: 3.1402\n",
            " D_img [d_img_gan_loss]: 1.1424\n",
            "t = 310 / 1000000\n",
            " G [L1_pixel_loss]: 0.9548\n",
            " G [bbox_pred]: 0.5061\n",
            " G [ac_loss]: 0.3045\n",
            " G [g_gan_obj_loss]: 0.0163\n",
            " G [g_gan_img_loss]: 0.0091\n",
            " G [total_loss]: 1.7909\n",
            " D_obj [d_obj_gan_loss]: 0.6713\n",
            " D_obj [d_ac_loss_real]: 4.0878\n",
            " D_obj [d_ac_loss_fake]: 3.0455\n",
            " D_img [d_img_gan_loss]: 1.1194\n",
            "t = 320 / 1000000\n",
            " G [L1_pixel_loss]: 0.7936\n",
            " G [bbox_pred]: 0.5020\n",
            " G [ac_loss]: 0.2782\n",
            " G [g_gan_obj_loss]: 0.0188\n",
            " G [g_gan_img_loss]: 0.0087\n",
            " G [total_loss]: 1.6013\n",
            " D_obj [d_obj_gan_loss]: 0.6890\n",
            " D_obj [d_ac_loss_real]: 3.9641\n",
            " D_obj [d_ac_loss_fake]: 2.7822\n",
            " D_img [d_img_gan_loss]: 1.1780\n",
            "t = 330 / 1000000\n",
            " G [L1_pixel_loss]: 0.9621\n",
            " G [bbox_pred]: 0.4924\n",
            " G [ac_loss]: 0.3049\n",
            " G [g_gan_obj_loss]: 0.0206\n",
            " G [g_gan_img_loss]: 0.0087\n",
            " G [total_loss]: 1.7887\n",
            " D_obj [d_obj_gan_loss]: 1.0835\n",
            " D_obj [d_ac_loss_real]: 3.9820\n",
            " D_obj [d_ac_loss_fake]: 3.0486\n",
            " D_img [d_img_gan_loss]: 1.2027\n",
            "t = 340 / 1000000\n",
            " G [L1_pixel_loss]: 0.9029\n",
            " G [bbox_pred]: 0.4767\n",
            " G [ac_loss]: 0.3038\n",
            " G [g_gan_obj_loss]: 0.0271\n",
            " G [g_gan_img_loss]: 0.0096\n",
            " G [total_loss]: 1.7201\n",
            " D_obj [d_obj_gan_loss]: 1.2236\n",
            " D_obj [d_ac_loss_real]: 4.0403\n",
            " D_obj [d_ac_loss_fake]: 3.0377\n",
            " D_img [d_img_gan_loss]: 1.0981\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}